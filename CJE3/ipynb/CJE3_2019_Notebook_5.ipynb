{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学生用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019年度知識情報演習III（後半）ノートブック 5\n",
    "本ノートブックでは、情報検索システムにおける索引付け処理と順位付け処理のプログラムを完成させることで、比較的大きなプログラムを書けるようになることを目標とします。\n",
    "\n",
    "## 完了の目安\n",
    "本ノートブックは、後半第5回（今回）の終わりか、レポート提出締切日までに完了することを目安にしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ノートブック 4の演習内容をTAか教員に確認してもらってから、ノートブック 5に進んでください。ただし確認待ちの間に先に進めるのは構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 【重要】本演習の学習方針\n",
    "\n",
    "プログラミングでは、ある問題を解決するための方法が複数存在するのが普通です。しかし本演習では、Pythonを使ったプログラミングにおいて**標準的な実装方法**を身につけることを重視します。したがって、以下の点に注意して演習を進めてください。\n",
    "\n",
    "- プログラムを作成する際に、ノートブックで説明されたやり方に沿って実装すること\n",
    "  - TAや教員もノートブックで説明した実装方法が実践できているかを確認します\n",
    "- 外部ウェブサイトなどで示された別の実装方法を参照した場合、プログラムが正常に機能したとしても**やり直しをしてもらう場合がある**\n",
    "- 情報検索の仕組みを理解するために、便利なライブラリを意図的に使わない場合がある\n",
    "- 情報検索システムの理解に直接関係ない複雑な処理に関して、便利なライブラリを使用する場合がある\n",
    "- その他の関連情報については、参照URLを示すので、授業外活動として各自学習しておくこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 【重要】本演習の実行環境\n",
    "\n",
    "プログラミングは、プログラムを実行する環境によって動作や出力結果が異なる場合があります。全学計算機システムの実行環境は以下の通りです。\n",
    "\n",
    "- OS : Ubuntu 16.04 LTS\n",
    "- Python : Anaconda Python 3 ver 5.2.0\n",
    "- JupyterLab : 0.32.1\n",
    "- Janome : 0.36\n",
    "\n",
    "研究室や個人のパソコンを使った場合、上記の実行環境と異なると同じ結果が出ない場合やエラーが出る（あるいは出ない）場合がありますので、注意が必要です。本演習では上記実行環境以外の動作に関するサポートは行いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 演習用フォルダ構成の確認\n",
    "作業を始める前に、演習用のフォルダが以下の構成になっていることを確認してください。  \n",
    "\n",
    "\n",
    "```\n",
    "ホームフォルダ\n",
    "  ┗ CJE3\n",
    "    ┣ ipynb\n",
    "      ┣ CJE3_2019_Notebook_0.ipynb\n",
    "      ┣ CJE3_2019_Notebook_1.ipynb\n",
    "      ┣ CJE3_2019_Notebook_2.ipynb\n",
    "      ┣ CJE3_2019_Notebook_3.ipynb\n",
    "      ┣ CJE3_2019_Notebook_4.ipynb\n",
    "      ┣ CJE3_2019_Notebook_5.ipynb（新規ファイル）\n",
    "      ┗ images\n",
    "          ┗ nested_dict.png\n",
    "    ┣ data\n",
    "      ┣ sample_doc1.txt\n",
    "      ┣ ...\n",
    "      ┗ sample_doc10.txt\n",
    "    ┗ index\n",
    "        ┣ index.txt\n",
    "        ┗ index2.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 文書の索引付け処理の完成\n",
    "\n",
    "索引付け処理では、以下の手続きを順に行なっていきます。\n",
    "\n",
    "1. 初期設定\n",
    "  1. ライブラリの読み込み\n",
    "  1. フォルダやファイル名の設定\n",
    "  1. 各種辞書オブジェクトの初期化\n",
    "1. 不用語削除ルールの定義\n",
    "  1. 正規表現\n",
    "  1. 不用語リスト\n",
    "1. 文書ファイルの処理\n",
    "  1. 分かち書き\n",
    "  1. 不用語処理\n",
    "  1. 索引語の追加\n",
    "1. 索引語の重み付け\n",
    "  1. 総文書数の算出\n",
    "  1. 文書頻度の算出\n",
    "1. 索引ファイルの書き出し\n",
    "  1. `TFIDF`値の算出\n",
    "  1. 索引語と重みデータの出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成させるためのヒント\n",
    "- コメント行に沿って、少しずつプログラムを書いていく\n",
    "- 確認用出力のコードを途中に挿入し、プログラムの動作を確認しながら進める\n",
    "- わからない箇所があったら、Notebook1-4の説明を熟読する（`Ctrl+F`でノートブック内を検索できます）\n",
    "\n",
    "> このプログラムは、100行近くある大きなプログラムですが、これまで学んだことを総動員すれば実装可能ですので、これまでの内容を復習しながら、完成させましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数名一覧（例）\n",
    "\n",
    "変数名は自由に設定して構いませんが、もし考えるのが大変な場合は、以下の変数名を使ってください。これまでの演習で用いてきたものがほとんどですが、一部変更してあります。\n",
    "\n",
    "|変数名|変数の役割|\n",
    "|:--:|:--:|\n",
    "|`DATA`|データフォルダのパス|\n",
    "|`df`|文書頻度（df）用辞書オブジェクト|\n",
    "|`doc`|（forループ内）辞書オブジェクトのキーとしてのある文書（＝ファイル名）|\n",
    "|`docs`|文書名一覧用辞書オブジェクト|\n",
    "|`doc_size`|文書総数（$N$）|\n",
    "|`f`|読み込み用に`open`したファイルのショートカット|\n",
    "|`f2`|書き込み用に`open`したファイルのショートカット|\n",
    "|`INDEX`|索引ファイル保存用フォルダのパス|\n",
    "|`idf`|逆文書頻度（idf）用辞書オブジェクト|\n",
    "|`index_words`|索引語の辞書オブジェクト|\n",
    "|`index_file`|索引ファイル名|\n",
    "|`line`|（forループ内）ファイル内のある行|\n",
    "|`pattern`|不用語（1文字）マッチ用パターン|\n",
    "|`stopwords`|不用語リスト用辞書オブジェクト|\n",
    "|`t`|分かち書きクラスのショートカット|\n",
    "|`tfidf`|`TFIDF`値|\n",
    "|`token`|分かち書きされたある語のオブジェクト|\n",
    "|`tokens`|分かち書きの出力全体|\n",
    "|`word`|（forループ内）辞書オブジェクトのキーとしてのある語|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# 1. 初期設定\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. ライブラリの読み込み\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "import math\n",
    "\n",
    "# 分かち書きクラスの初期化\n",
    "t = Tokenizer()\n",
    "\n",
    "### B. フォルダやファイル名の設定\n",
    "\n",
    "# データフォルダの設定\n",
    "DATA = \"../data\"\n",
    "# 索引ファイルの指定\n",
    "INDEX = \"../index\"\n",
    "index_file = INDEX + \"/index3.txt\"\n",
    "\n",
    "### C. 各種辞書オブジェクトの初期化\n",
    "\n",
    "# 索引語用辞書オブジェクトの初期化\n",
    "index_words = {}\n",
    "# 不要語辞書の初期化\n",
    "stopwords = {} \n",
    "# ファイル名保存用辞書オブジェクト\n",
    "docs = {}\n",
    "# df用辞書オブジェクトの初期化\n",
    "df = {}\n",
    "# idf値用辞書オブジェクト\n",
    "idf = {}\n",
    "\n",
    "#---------------------------------------------\n",
    "# 2. 不用語削除ルールの定義\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. 正規表現\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "\n",
    "### B. 不要語リスト\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "#---------------------------------------------\n",
    "# 3. 文書ファイルの処理\n",
    "#---------------------------------------------\n",
    "\n",
    "# DATAフォルダに含まれるファイルを一つずつ処理\n",
    "for filename in os.listdir(DATA):\n",
    "    # ファイルを読み込みモードで開く\n",
    "    f = open(DATA + '/' + filename, 'r')\n",
    "    # ファイルを1行ずつ処理\n",
    "    for line in f:\n",
    "        ### A. 分かち書き\n",
    "        tokens = t.tokenize(line)\n",
    "        # 分かち書きされた語オブジェクトの処理\n",
    "        for token in tokens:\n",
    "            ### B. 不用語処理\n",
    "            # 正規表現\n",
    "            if pattern.match(token.surface):\n",
    "                continue\n",
    "\n",
    "            # 不用語リスト\n",
    "            if token.surface in stopwords:\n",
    "                continue\n",
    "\n",
    "            ### C. 索引語の追加\n",
    "            if token.surface in index_words:\n",
    "                # もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "                if filename in index_words[token.surface]:\n",
    "                    index_words[token.surface][filename] += 1\n",
    "\n",
    "                # さもなくば\n",
    "                else:\n",
    "                    index_words[token.surface][filename] = 1\n",
    "\n",
    "            # さもなくば\n",
    "            else:\n",
    "                index_words[token.surface] = {}\n",
    "                index_words[token.surface][filename] = 1\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# 4. 索引語の重み付け\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. 総文書数の算出\n",
    "\n",
    "# 索引語を一つずつ処理\n",
    "for word in index_words:\n",
    "    # 文書を一つずつ処理\n",
    "    for doc in index_words[word]:\n",
    "        # もし辞書オブジェクトに文書がないなら追加\n",
    "        if doc not in docs:\n",
    "            docs[doc] = 1\n",
    "\n",
    "# 総文書数（N）を求める\n",
    "docs_size = len(docs)\n",
    "\n",
    "### B. 文書頻度の算出\n",
    "\n",
    "# 索引語を一つずつ処理\n",
    "for word in index_words:\n",
    "    # 文書数を取得し、dfに代入\n",
    "    df[word] = len(index_words[word])\n",
    "\n",
    "### C. 逆文書頻度の算出\n",
    "\n",
    "# dfのキー（索引語）を一つずつ処理\n",
    "for word in df:\n",
    "    # idf値を計算し、格納\n",
    "    idf[word] = math.log((docs_size / df[word])+1)\n",
    "\n",
    "#---------------------------------------------\n",
    "# 5. 索引ファイルの書き出し\n",
    "#---------------------------------------------\n",
    "\n",
    "# ファイルを書き込みモードで開く\n",
    "f2 = open(index_file, 'w')\n",
    "\n",
    "# ソートした索引語を一つずつ処理\n",
    "for word in sorted(index_words):\n",
    "    # ソートした文書を一つずつ処理\n",
    "    for doc in sorted(index_words[word]):\n",
    "        ### A. tfidf値の算出\n",
    "        tfidf = index_words[word][doc] * idf[word]\n",
    "        ### B. 索引語と重みデータの出力\n",
    "        f2.write(word + '\\t' + doc +'\\t' + str(index_words[word][doc]) +'\\t' + str(idf[word]) + '\\t' + str(tfidf) + '\\n')\n",
    "\n",
    "# ファイルを閉じる\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "`index`フォルダに、`index3.txt`というファイルが生成され、 `sample_doc1.txt` 〜 `sample_doc10.txt`のファイル名と出現する索引語が含まれており、各文書における索引語の重みデータが含まれていたら、成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 索引付けプログラムの説明文を書く\n",
    "\n",
    "それでは、文書を索引付けするプログラムの説明文を書きましょう。説明文を書く上での注意点はNotebook1を参照してください。\n",
    "\n",
    "\n",
    "> 6: ディレクトリ内にあるファイルを読み込むための`os`ライブラリを読み込む\n",
    "\n",
    "> 7: 分かち書きをするライブラリ`janome.tokenizer`から、分かち書きクラス`Tokenizer`を読み込む\n",
    "\n",
    "> 8: 正規表現をあつかう`re`パッケージを読み込む\n",
    "\n",
    "> 9: 数学処理を扱う`math`ライブラリをインポート\n",
    "\n",
    "> 12: 分かち書きクラス`Tokenizer`を初期化し、クラスのショートカットを変数`t`に格納する\n",
    "\n",
    "> 17: CJE3の`data`フォルダを指定\n",
    "\n",
    "> 19: CJE3の`index`フォルダを指定\n",
    "\n",
    "> 20: 出力ファイルのフォルダとファイル名を指定\n",
    "\n",
    "> 25: 索引語用辞書オブジェクト`index_words`を初期化\n",
    "\n",
    "> 27: 不要語辞書`stopworrds`を初期化\n",
    "\n",
    "> 29: ファイル名保存用辞書オブジェクト`docs`を初期化\n",
    "\n",
    "> 31: df用辞書オブジェクト`df`の初期化\n",
    "\n",
    "> 33: idf用辞書オブジェクト`idf`の初期化\n",
    "\n",
    "> 40: 不要語として削除したい文字列のパターンを定義し, 変数`pattern`に保存（今回はひらがな1文字のみのものと句読点）\n",
    "\n",
    "> 43~44: 不要語として削除したい文字列をキー, 値を1として`stopwords`にレコードを追加\n",
    "\n",
    "> 51: `os`ライブラリの`listdir`メソッドを使って、変数DATAが参照するフォルダに含まれるファイル名のリストを取得。最初のファイル名を変数`filename`に格納。forメソッドを使って以下の処理をリストの終わりまで繰り返す\n",
    "\n",
    "> 53: 変数`filename`を`open`メソッドを使って読み出し専用モード（'r'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数`f`に格納する\n",
    "\n",
    "> 55: 変数`f`の内容を1行ずつ取り出し変数`line`に格納する。これを最終行まで繰り返す。\n",
    "\n",
    "> 57: 変数lineに格納された行を、`tokenize`メソッドで分かち書きして、その結果を変数`tokens`に格納\n",
    "\n",
    "> 59: 変数`tokens`に格納された要素を1つずつ取り出し変数`token`に格納する。これを最後の要素になるまで繰り返す。\n",
    "\n",
    "> 62: `surface`メソッドを使って得られる単語本体が`pattern`にマッチした場合の処理\n",
    "\n",
    "> 63: それ以降の処理をスキップする\n",
    "\n",
    "> 66: `surface`メソッドを使って得られる単語本体をキーとするレコードが`stopwords`に存在していた場合の処理\n",
    "\n",
    "> 67: それ以降の処理をスキップする\n",
    "\n",
    "> 70: `surface`メソッドを使って得られる単語本体ををキーとするレコードが`index_words`に存在していた場合の処理\n",
    "\n",
    "> 72: もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していた場合の処理\n",
    "\n",
    "> 73: 語とファイル名をキーとするレコードの値を1増やす\n",
    "\n",
    "> 76: 72行目が`false`だった場合\n",
    "\n",
    "> 77: 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "> 80: 70行目が`false`だった場合\n",
    "\n",
    "> 81: そのキーを使って`index_words`に無名辞書オブジェクトを初期化\n",
    "\n",
    "> 82: 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "> 92: `index_words`のキーを変数`word`として1つずつ取り出し最後の要素まで繰り返す\n",
    "\n",
    "> 94: 索引語キーが指す入れ子オブジェクト内のキーを変数`doc`として取り出しを一つずつ処理する\n",
    "\n",
    "> 96: `not in`メソッドを使って、変数`doc`がもつファイル名が、辞書オブジェクト`docs`に存在しない時の処理\n",
    "\n",
    "> 97: ファイル名をキー, 値を1としたレコードを`docs`に追加する\n",
    "\n",
    "> 100: `len`メソッドを使って`docs`の要素数を取得し, 変数`docs_size`に総文書数として保存する\n",
    "\n",
    "> 105: `index_words`のキーを変数`word`として1つずつ取り出し、最後の要素まで繰り返す\n",
    "\n",
    "> 107: `len`メソッドを使って`word`が指す入れ子辞書の要素数を取得し, `df`に代入する\n",
    "\n",
    "> 112: `df`のキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 114: idf値を定義に基づいて計算する\n",
    "\n",
    "> 121: 変数`index_file`を`open`メソッドを使って書き出し専用モード（'w'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数`f2`に格納する\n",
    "\n",
    "> 124: `index_words`のソートされたキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 126: そのキーが指す入れ子オブジェクト内のソートされたキーを変数`doc`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 127: tfidf値を定義に基づいて計算する\n",
    "\n",
    "> 130: `write`メソッドを使って索引語と重みデータをファイルに書き出す\n",
    "\n",
    "> 133~134: 開いたファイルを`close`メソッドを使って閉じる\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 文書の順位付け処理の完成\n",
    "\n",
    "順位付け処理では、以下の手続きを順に行なっていきます。\n",
    "\n",
    "1. 初期設定\n",
    "  1. ライブラリの読み込み\n",
    "  1. フォルダやファイル名の設定\n",
    "  1. 各種辞書オブジェクトの初期化\n",
    "1. 索引ファイルの読み込み\n",
    "  1. 行末処理と要素の分割\n",
    "  1. 重み値の取得\n",
    "  1. データフレームの作成\n",
    "1. 検索質問の処理\n",
    "  1. 不用語削除ルールの定義\n",
    "  1. 分かち書き\n",
    "  1. 不用語処理\n",
    "  1. 検索語の重み付け\n",
    "  1. データフレームの作成\n",
    "1. 類似度の計算\n",
    "  1. 順位付け対象文書の同定\n",
    "  1. 余弦関数の分子の算出\n",
    "  1. 余弦関数の分母の算出\n",
    "  1. 余弦関数の算出\n",
    "1. 順位付け結果の出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完成させるためのヒント\n",
    "\n",
    "- コメント行に沿って、少しずつプログラムを書いていく\n",
    "- 確認用出力のコードを途中に挿入し、プログラムの動作を確認しながら進める\n",
    "- わからない箇所があったら、Notebook1-4の説明を熟読する（`Ctrl+F`でノートブック内を検索できます）\n",
    "- 注意：検索質問の不要語削除ルールは、索引付け処理と同一のものを使う必要があります\n",
    "\n",
    "> このプログラムは、150行近くある大きなプログラムですが、これまで学んだことを総動員すれば実装可能ですので、これまでの内容を復習しながら、完成させましょう！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数名一覧（例）\n",
    "\n",
    "変数名は自由に設定して構いませんが、もし考えるのが大変な場合は、以下の変数名を使ってください。これまでの演習で用いてきたものがほとんどですが、一部変更してあります。\n",
    "\n",
    "|変数名|変数の役割|\n",
    "|:--:|:--:|\n",
    "|`cosine`|余弦関数（コサイン）値|\n",
    "|`denominator`|余弦関数の分母|\n",
    "|`doc`|ある文書|\n",
    "|`doc_value`|余弦関数の分母内の索引語ベクトルの大きさの積|\n",
    "|`doc_vec`|文書のベクトル|\n",
    "|`f`|読み込み用に`open`したファイルのショートカット|\n",
    "|`idf`|ある逆文書頻度（idf）値|\n",
    "|`idf_scores`|逆文書頻度（idf）値用辞書オブジェクト|\n",
    "|`INDEX`|索引ファイル保存用フォルダのパス|\n",
    "|`index_file`|索引ファイル名|\n",
    "|`index_word`|ある索引語|\n",
    "|`line`|（forループ内）ファイル内のある行|\n",
    "|`numerator`|余弦関数の分子|\n",
    "|`pattern`|不用語（1文字）マッチ用パターン|\n",
    "|`query`|検索質問|\n",
    "|`query_file`|検索質問データフレーム用ファイル名|\n",
    "|`query_table`|`query_tfidf`から生成したデータフレーム|\n",
    "|`query_tf`|検索質問内検索語の`TF`値用辞書オブジェクト|\n",
    "|`query_tfidf`|検索質問内検索語の`TFIDF`値用辞書オブジェクト|\n",
    "|`query_value`|余弦関数の分母内の検索語ベクトルの大きさの積|\n",
    "|`query_vec`|検索質問のベクトル|\n",
    "|`query_word`|ある検索語|\n",
    "|`query_words`|検索語用辞書オブジェクト|\n",
    "|`ranking_docs`|順位付け対象文書用辞書オブジェクト|\n",
    "|`split_line`|分割した行の要素配列|\n",
    "|`stopwords`|不用語リスト用辞書オブジェクト|\n",
    "|`t`|分かち書きクラスのショートカット|\n",
    "|`tfidf`|ある`TFIDF`値|\n",
    "|`tfidf_scores`|文書内索引語の`TFIDF`値用辞書オブジェクト|\n",
    "|`tfidf_table`|`tfidf_scores`から生成したデータフレーム|\n",
    "|`token`|分かち書きされたある語のオブジェクト|\n",
    "|`tokens`|分かち書きの出力全体|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sample_doc1.txt', 0.29266813605734854),\n",
       " ('sample_doc4.txt', 0.10005073155879705),\n",
       " ('sample_doc10.txt', 0.08055463906680686),\n",
       " ('sample_doc8.txt', 0.0663796541529564),\n",
       " ('sample_doc7.txt', 0.06025061996297909),\n",
       " ('sample_doc9.txt', 0.03242115991853381),\n",
       " ('sample_doc5.txt', 0.026802662255599862),\n",
       " ('sample_doc2.txt', 0.02341033093807391),\n",
       " ('sample_doc3.txt', 0.020156833547929343)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---------------------------------------------\n",
    "# 1. 初期設定\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. ライブラリの読み込み\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# 分かち書きクラスの初期化\n",
    "t = Tokenizer()\n",
    "\n",
    "### B. フォルダやファイル名の設定\n",
    "\n",
    "# 索引ファイルの指定\n",
    "INDEX = \"../index\"\n",
    "index_file = INDEX + \"/index3.txt\"\n",
    "\n",
    "### C. 各種辞書オブジェクトの初期化\n",
    "\n",
    "# 索引語idf値の辞書オブジェクト\n",
    "idf_scores = {}\n",
    "# 索引語tfidf値の辞書オブジェクト\n",
    "tfidf_scores = {}\n",
    "# 検索語tf値の辞書オブジェクト\n",
    "query_tf = {}\n",
    "# 検索語tfidf値の辞書オブジェクト\n",
    "query_tfidf = {}\n",
    "# 不要語辞書オブジェクト\n",
    "stopwords = {}\n",
    "# 検索語の辞書オブジェクト\n",
    "query_words = {}\n",
    "# 順位付け対象文書用辞書オブジェクト\n",
    "ranking_docs = {}\n",
    "\n",
    "#---------------------------------------------\n",
    "# 2. 索引ファイルの読み込み\n",
    "#---------------------------------------------\n",
    "\n",
    "# 索引ファイルを読み込みモードで開く\n",
    "f = open(index_file, 'r')\n",
    "\n",
    "# 索引ファイルを1行づつ処理\n",
    "for line in f:\n",
    "    ### A. 行末処理と要素の分割\n",
    "\n",
    "    # 行末の改行文字除去\n",
    "    line = line.rstrip()\n",
    "    # 行の分割\n",
    "    split_line = line.split(\"\\t\")\n",
    "    \n",
    "    # 配列要素の取得\n",
    "    word = split_line[0] \n",
    "    doc = split_line[1] \n",
    "    idf = float(split_line[3])\n",
    "    tfidf = float(split_line[4])\n",
    "\n",
    "    \n",
    "    ### B. 重み値の取得\n",
    "    idf_scores[word] = idf\n",
    "    # 辞書オブジェクトへの代入（改良版）\n",
    "    if word in tfidf_scores:\n",
    "        tfidf_scores[word][doc] = tfidf\n",
    "        \n",
    "    else:\n",
    "        tfidf_scores[word] = {}\n",
    "        tfidf_scores[word][doc] = tfidf\n",
    "\n",
    "        \n",
    "### C. tfidf_scoresからデータフレームを作成\n",
    "tfidf_table = pd.DataFrame(tfidf_scores)\n",
    "\n",
    "# NaNを0に置き換える\n",
    "tfidf_table = tfidf_table.fillna(0)\n",
    "\n",
    "#---------------------------------------------\n",
    "# 3. 検索質問の処理\n",
    "#---------------------------------------------\n",
    "\n",
    "# 検索質問\n",
    "query = '吾輩は猫である'\n",
    "# 検索質問データフレーム用ファイル名\n",
    "query_file = 'query'\n",
    "\n",
    "### A. 不用語削除ルールの定義\n",
    "\n",
    "# 不要語としてマッチしたいパターンの定義\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "# 不要語の追加\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "### B. 検索質問の分かち書き\n",
    "tokens = t.tokenize(query)\n",
    "\n",
    "# 分かち書きされた語オブジェクトの処理\n",
    "for token in tokens:\n",
    "    ### C. 不用語処理\n",
    "    # 正規表現\n",
    "    if pattern.match(token.surface):\n",
    "        continue\n",
    "\n",
    "    # 不用語リスト\n",
    "    if token.surface in stopwords:\n",
    "        continue\n",
    "\n",
    "    #  検索語の追加とtf値の加算\n",
    "    if token.surface in query_words:\n",
    "        query_words[token.surface] += 1\n",
    "    else:\n",
    "        query_words[token.surface] = 1\n",
    "\n",
    "### D. 検索語の重み付け\n",
    "# 索引語を一つずつ処理\n",
    "for word in idf_scores:\n",
    "    # 索引語をキーとした検索質問tf値用とtfidf値用オブジェクトを初期化\n",
    "    query_tf[word] = {}  \n",
    "    # キー：索引語、擬似文書名「query」、値：0\n",
    "    query_tf[word][query_file] = 0   \n",
    "\n",
    "# 検索語を一つずつ処理\n",
    "for query_word in query_words:   \n",
    "    # 検索語==索引語なレコードのtf値をquery_wordsから代入\n",
    "    for index_word in idf_scores:\n",
    "        if query_word == index_word:\n",
    "            query_tf[query_word][query_file] = query_words[query_word]\n",
    "\n",
    "# 検索語の出現頻度（`tf`値）と`idf`値を使って、`tfidf`値を算出する\n",
    "for word in query_tf:\n",
    "    for q_value in query_tf[word]:\n",
    "        tf = query_tf[word][q_value]\n",
    "        idf = idf_scores[word]\n",
    "        tfidf = tf * idf\n",
    "        query_tfidf[word] = tfidf\n",
    "\n",
    "### D. query_tfidfからデータフレームを作成\n",
    "query_table = pd.DataFrame(query_tfidf, index=['query',])\n",
    "\n",
    "#---------------------------------------------\n",
    "# 4. 類似度の計算と順位付け\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. 順位付け対象文書の同定\n",
    "for query_word in query_words:\n",
    "    for doc in tfidf_scores[query_word]:\n",
    "        # 値`1`で初期化\n",
    "        ranking_docs[doc] = 1\n",
    "\n",
    "# 対象文書を一つずつ処理\n",
    "for doc in ranking_docs:\n",
    "    ### B. 余弦関数の分子の算出\n",
    "    \n",
    "    # 分子変数の初期化\n",
    "    numerator = 0\n",
    "    # 文書データフレームからdocにマッチする行のデータを取得\n",
    "    doc_vec = tfidf_table.loc[doc]\n",
    "    # 検索質問データフレームからquery_fileにマッチする行のデータを取得\n",
    "    query_vec = query_table.loc['query']\n",
    "    # 検索質問ベクトルの要素を添字を使って巡回\n",
    "    for i in range(len(query_vec.values)):\n",
    "        # query_vecとdoc_vecのi番目の要素を掛け合わせて、分子変数に足していく\n",
    "        i_value = query_vec.values[i] * doc_vec.values[i]\n",
    "        numerator += i_value\n",
    "\n",
    "    ### C. 余弦関数の分母の算出\n",
    "    \n",
    "    # 分母変数の初期化\n",
    "    denominator = 0\n",
    "    # 検索語ベクトル積の変数の初期化\n",
    "    query_value = 0\n",
    "    # 索引語ベクトル積の変数の初期化\n",
    "    doc_value = 0\n",
    "    # 検索質問ベクトルの要素を添字を使って巡回\n",
    "    for i in range(len(query_vec.values)):\n",
    "        # 検索質問ベクトルのi番目の値の二乗をquery_valueに加算していく\n",
    "        query_value += query_vec.values[i] ** 2\n",
    "        # 文書ベクトルのi番目の値の二乗をdoc_valueに加算していく\n",
    "        doc_value += doc_vec.values[i] ** 2\n",
    "    \n",
    "    # query_valueとdoc_valueの平方根を掛け合わせて分母とする\n",
    "    denominator = math.sqrt(query_value) * math.sqrt(doc_value)\n",
    "    \n",
    "    ### D. 余弦関数の算出\n",
    "    cosine = numerator / denominator\n",
    "    \n",
    "    # ranking_docsへ値を代入\n",
    "    ranking_docs[doc] = cosine\n",
    "\n",
    "#---------------------------------------------\n",
    "# 5. 順位付け結果の出力\n",
    "#---------------------------------------------\n",
    "\n",
    "# 類似度の高い順に文書を表示\n",
    "sorted(ranking_docs.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "出力例（不要語の設定次第で異なります）\n",
    "\n",
    "```\n",
    "[('sample_doc1.txt', 0.29266813605734854),\n",
    " ('sample_doc4.txt', 0.10005073155879705),\n",
    " ('sample_doc10.txt', 0.080554639066806863),\n",
    " ('sample_doc8.txt', 0.066379654152956394),\n",
    " ('sample_doc7.txt', 0.060250619962979091),\n",
    " ('sample_doc9.txt', 0.03242115991853381),\n",
    " ('sample_doc5.txt', 0.026802662255599862),\n",
    " ('sample_doc2.txt', 0.02341033093807391),\n",
    " ('sample_doc3.txt', 0.020156833547929343)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 順位付けプログラムの説明文を書く\n",
    "\n",
    "それでは、本Notebookの最後の演習として、文書を順位付けするプログラムの説明文を書きましょう。説明文を書く上での注意点はNotebook1を参照してください。\n",
    "\n",
    "\n",
    "> 6: ディレクトリ内にあるファイルを読み込むための`os`ライブラリを読み込む\n",
    "\n",
    "> 7: 分かち書きをするライブラリ`janome.tokenizer`から、分かち書きクラス`Tokenizer`を読み込む\n",
    "\n",
    "> 8: 正規表現をあつかう`re`パッケージを読み込む\n",
    "\n",
    "> 9: 数学処理を扱う`math`ライブラリをインポート\n",
    "\n",
    "> 10: データ分析ライブラリのpandasをpdとしてインポート\n",
    "\n",
    "> 13: 分かち書きクラス`Tokenizer`を初期化し、クラスのショートカットを変数`t`に格納する\n",
    "\n",
    "> 18: CJE3の`index`フォルダを指定\n",
    "\n",
    "> 19: 出力ファイルのフォルダとファイル名を指定\n",
    "\n",
    "> 24: 索引語idf値用辞書オブジェクト`idf_scores`を初期化\n",
    "\n",
    "> 26: 索引語tfidf値用辞書オブジェクト`tfidf_scores`を初期化\n",
    "\n",
    "> 28: 検索語tf値用辞書オブジェクト`query_tf`を初期化\n",
    "\n",
    "> 30: 検索語tfidf値用辞書オブジェクト`query_tfidf`の初期化\n",
    "\n",
    "> 32: 不要語辞書オブジェクト`stopworrds`を初期化\n",
    "\n",
    "> 34: 検索語辞書オブジェクト`query_words`を初期化\n",
    "\n",
    "> 36: 順位付け対象文書用辞書オブジェクト`ranking_docs`を初期化\n",
    "\n",
    "> 43: 変数`index_file`を`open`メソッドを使って読み出し専用モード（'r'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数`f`に格納する\n",
    "\n",
    "> 46: 変数`f`の内容を1行ずつ取り出し変数`line`に格納する。これを最終行まで繰り返す。\n",
    "\n",
    "> 50: `rstrip`メソッドを使って`line`に含まれている行末の改行文字を削除\n",
    "\n",
    "> 52: `split`メソッドを使ってlineを`\\t`ごとに区切りそれを配列`split_words`に保存\n",
    "\n",
    "> 55~58: `split_line`の要素を取得しそれぞれ`word`, `doc`, `idf`, `tfidf`として保存。その際`idf`と`tfidf`はfloat型でキャストする\n",
    "\n",
    "> 62: `word`をキー, `idf`を値とするレコードを`idf_scores`に追加\n",
    "\n",
    "> 64: `tfidf_scores`に`word`をキーとするレコードが存在していた場合の処理\n",
    "\n",
    "> 65: `word`と`doc`をキー, `tfidf`を値とするレコードを`tfidf_scores`に追加する\n",
    "\n",
    "> 67: 64行目が`false`だった場合の処理\n",
    "\n",
    "> 68: `word`をキーとして`tfidf_scores`に無名辞書オブジェクトを初期化\n",
    "\n",
    "> 69: `word`と`doc`をキー, `tfidf`を値とするレコードを`tfidf_scores`に追加する\n",
    "\n",
    "> 73: `tfidf_scores`を、`pd.DataFrame`メソッドを用いてデータフレームに変換し、変数`tfidf_table`に保存\n",
    "\n",
    "> 76: `fillna`メソッドを使ってNaNを0に置き換える\n",
    "\n",
    "> 83: `query`を定義\n",
    "\n",
    "> 85: 検索質問データフレーム用ファイル名を\"query\"として`query_file`に保存\n",
    "\n",
    "> 90: 不要語として削除したい文字列のパターンを定義し, 変数`pattern`に保存（今回はひらがな1文字のみのものと句読点）\n",
    "\n",
    "> 92~93: 不要語として削除したい文字列をキー, 値を1として`stopwords`にレコードを追加\n",
    "\n",
    "> 96: 変数`query`に格納された質問文を、`tokenize`メソッドで分かち書きして、その結果を変数`tokens`に格納\n",
    "\n",
    "> 99: 変数`tokens`に格納された要素を1つずつ取り出し変数`token`に格納する。これを最後の要素になるまで繰り返す。\n",
    "\n",
    "> 102: `surface`メソッドを使って得られる単語本体が`pattern`にマッチした場合の処理\n",
    "\n",
    "> 103: それ以降の処理をスキップする\n",
    "\n",
    "> 106: `surface`メソッドを使って得られる単語本体をキーとするレコードが`stopwords`に存在していた場合の処理\n",
    "\n",
    "> 107: それ以降の処理をスキップする\n",
    "\n",
    "> 110: `surface`メソッドを使って得られる単語本体をキーとするレコードが`query_words`に存在していた場合の処理\n",
    "\n",
    "> 111: tf値を1増やす\n",
    "\n",
    "> 112: 110行目が`false`だったときの処理\n",
    "\n",
    "> 113: `surface`メソッドを使って得られる単語本体をキー, 値を1とするレコードを`query_words`に登録\n",
    "\n",
    "> 117: `idf_scores`のキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 119: 索引語をキーとした無名辞書オブジェクトを初期化\n",
    "\n",
    "> 121: 索引語, 擬似文書名をキー, 値を0としてレコード登録\n",
    "\n",
    "> 124: `query_words`のキーを変数`query_word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 126: `idf_scores`のキーを変数`index_word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 127: `query_word`と`index_word`が一致した場合の処理\n",
    "\n",
    "> 128: そのキーのtf値を`query_words`から`query_tf`に代入\n",
    "\n",
    "> 131: `query_tf`のキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 132: `query_tf`の`word`をキーとするレコードを変数`q_value`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 133~136: tf値, idf値を取り出しそれを用いてtfidf値を算出して`query_tfidf`に`word`をキー, `tfidf`を値としてレコード登録\n",
    "\n",
    "> 139: `uery_tfidf`からデータフレームを作成\n",
    "\n",
    "> 146: `query_words`のキーを変数`query_word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 147: `tfidf_scores`の`query_word`をキーとするレコードを変数`doc`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 148: `ranking_docs`に`doc`をキー, 値を1でレコード登録\n",
    "\n",
    "> 152: `ranking_docs`のキーを`doc`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 156: `numerator`の初期化\n",
    "\n",
    "> 158: `loc`メソッドを使って文書データフレームから`doc`にマッチする行のデータを取得し, `doc_vec`に保存\n",
    "\n",
    "> 160: `loc`メソッドを使って検索質問データフレームから`query_file`にマッチする行のデータを取得し, `query_vec`に保存\n",
    "\n",
    "> 162: `len`メソッドを使って`query_vec`の要素数を取得しその分だけ添字iを使って巡回する\n",
    "\n",
    "> 164~165: `query_vec`と`doc_vec`のi番目の要素を掛け合わせて、`numerator`に足していく\n",
    "\n",
    "> 170~174: 計算に使う変数を初期化しそれぞれ`denominator`, `query_value`, `doc_value`とする\n",
    "\n",
    "> 176: `len`メソッドを使って`query_vec`の要素数を取得しその分だけ添字iを使って巡回する\n",
    "\n",
    "> 178: `query_vec`のi番目の値の二乗を`query_value`に加算していく\n",
    "\n",
    "> 180: `doc_vec`のi番目の値の二乗を`doc_value`に加算していく\n",
    "\n",
    "> 183: `query_value`と`doc_value`の平方根を掛け合わせて分母とし, `denominator`に保存\n",
    "\n",
    "> 186: cosine類似度の定義に基づきそれを算出し`cosine`に保存\n",
    "\n",
    "> 189: `ranking_docs`の値を`cosine`で書き換える\n",
    "\n",
    "> 196: `sorted`メソッドに値でソートし降順で表示するオプションを追加し`ranking_docs`のレコードを表示する\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ## ここまで完成したら、以下のチェックポイントを自分で確認した上で、TAか教員に確認してもらいましょう。それが終わったら、レポート課題に取り組みましょう。\n",
    "\n",
    "### チェックポイント（索引付け・順位付け）\n",
    "\n",
    "- プログラムが期待通りに動作するか、索引ファイルは作成されているか、検索結果は妥当か\n",
    "- ノートブックで説明したやり方に沿ってプログラムが書かれているか（我流で書いていないか）\n",
    "- 変数の命名規則は適切か（きちんと意味のある変数名を使っているか）\n",
    "- 説明文にライブラリ・クラス・変数・メソッド・ループなどのキーワードを正確に使用しているか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright &copy; 2019. Hideo Joho and Haitao Yu. All rights reserved.\n",
    "\n",
    "無断複製・転載・配布行為を禁止します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
