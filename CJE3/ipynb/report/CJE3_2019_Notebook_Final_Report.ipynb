{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**学生用**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019年度知識情報演習III（後半）最終レポート\n",
    "本ノートブックは、知識情報演習III（後半）のレポート提出用です。\n",
    "\n",
    "- 課題1〜2は全員向けの課題です（未完成でも採点します）\n",
    "- 課題3は腕に覚えがある上級者向けの**任意課題**（加点対象）です\n",
    "- 課題4は全員向けのアンケート（任意）です\n",
    "- **↓の学籍番号と氏名が未記入のレポートは採点対象になりませんので、必ず記入してください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 学生情報\n",
    "\n",
    "- 学籍番号：201811552\n",
    "- 氏名：渡邉真悟\n",
    "\n",
    "## 宣誓\n",
    "\n",
    "私は本レポートの提出にあたり、以下の内容を誓います。\n",
    "\n",
    "- 本レポートは上記氏名の本人自身が作成した。\n",
    "- 他人のレポートやプログラムを写すなどを含む一切の不正行為をしていない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 提出方法\n",
    "\n",
    "1. ノートブックのダウンロード\n",
    "  - 左パネルファイル一覧 → `CJE3_2019_Notebook_Final_Report.ipynb`を右クリック → Download\n",
    "2. Manaba\n",
    "  - レポート → 後半最終レポート → ノートブックをアップロード\n",
    "\n",
    "**締切（火曜日組）：2020年2月4日17:00**（時間厳守）  \n",
    "**締切（水曜日組）：2020年2月12日17:00**（時間厳守）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題1-1：索引付け処理のプログラムを書きなさい\n",
    "\n",
    "- ノートブック5で作成したプログラムを清書したものです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# 1. 初期設定\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. ライブラリの読み込み\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "import math\n",
    "\n",
    "# 分かち書きクラスの初期化\n",
    "t = Tokenizer()\n",
    "\n",
    "### B. フォルダやファイル名の設定\n",
    "\n",
    "# データフォルダの設定\n",
    "DATA = \"../data\"\n",
    "# 索引ファイルの指定\n",
    "INDEX = \"../index\"\n",
    "index_file = INDEX + \"/index3.txt\"\n",
    "\n",
    "### C. 各種辞書オブジェクトの初期化\n",
    "\n",
    "# 索引語用辞書オブジェクトの初期化\n",
    "index_words = {}\n",
    "# 不要語辞書の初期化\n",
    "stopwords = {} \n",
    "# ファイル名保存用辞書オブジェクト\n",
    "docs = {}\n",
    "# df用辞書オブジェクトの初期化\n",
    "df = {}\n",
    "# idf値用辞書オブジェクト\n",
    "idf = {}\n",
    "\n",
    "#---------------------------------------------\n",
    "# 2. 不用語削除ルールの定義\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. 正規表現\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "\n",
    "### B. 不要語リスト\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "#---------------------------------------------\n",
    "# 3. 文書ファイルの処理\n",
    "#---------------------------------------------\n",
    "\n",
    "# DATAフォルダに含まれるファイルを一つずつ処理\n",
    "for filename in os.listdir(DATA):\n",
    "    # ファイルを読み込みモードで開く\n",
    "    f = open(DATA + '/' + filename, 'r')\n",
    "    # ファイルを1行ずつ処理\n",
    "    for line in f:\n",
    "        ### A. 分かち書き\n",
    "        tokens = t.tokenize(line)\n",
    "        # 分かち書きされた語オブジェクトの処理\n",
    "        for token in tokens:\n",
    "            ### B. 不用語処理\n",
    "            # 正規表現\n",
    "            if pattern.match(token.surface):\n",
    "                continue\n",
    "\n",
    "            # 不用語リスト\n",
    "            if token.surface in stopwords:\n",
    "                continue\n",
    "\n",
    "            ### C. 索引語の追加\n",
    "            if token.surface in index_words:\n",
    "                # もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "                if filename in index_words[token.surface]:\n",
    "                    index_words[token.surface][filename] += 1\n",
    "\n",
    "                # さもなくば\n",
    "                else:\n",
    "                    index_words[token.surface][filename] = 1\n",
    "\n",
    "            # さもなくば\n",
    "            else:\n",
    "                index_words[token.surface] = {}\n",
    "                index_words[token.surface][filename] = 1\n",
    "\n",
    "\n",
    "#---------------------------------------------\n",
    "# 4. 索引語の重み付け\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. 総文書数の算出\n",
    "\n",
    "# 索引語を一つずつ処理\n",
    "for word in index_words:\n",
    "    # 文書を一つずつ処理\n",
    "    for doc in index_words[word]:\n",
    "        # もし辞書オブジェクトに文書がないなら追加\n",
    "        if doc not in docs:\n",
    "            docs[doc] = 1\n",
    "\n",
    "# 総文書数（N）を求める\n",
    "docs_size = len(docs)\n",
    "\n",
    "### B. 文書頻度の算出\n",
    "\n",
    "# 索引語を一つずつ処理\n",
    "for word in index_words:\n",
    "    # 文書数を取得し、dfに代入\n",
    "    df[word] = len(index_words[word])\n",
    "\n",
    "### C. 逆文書頻度の算出\n",
    "\n",
    "# dfのキー（索引語）を一つずつ処理\n",
    "for word in df:\n",
    "    # idf値を計算し、格納\n",
    "    idf[word] = math.log((docs_size / df[word])+1)\n",
    "\n",
    "#---------------------------------------------\n",
    "# 5. 索引ファイルの書き出し\n",
    "#---------------------------------------------\n",
    "\n",
    "# ファイルを書き込みモードで開く\n",
    "f2 = open(index_file, 'w')\n",
    "\n",
    "# ソートした索引語を一つずつ処理\n",
    "for word in sorted(index_words):\n",
    "    # ソートした文書を一つずつ処理\n",
    "    for doc in sorted(index_words[word]):\n",
    "        ### A. tfidf値の算出\n",
    "        tfidf = index_words[word][doc] * idf[word]\n",
    "        ### B. 索引語と重みデータの出力\n",
    "        f2.write(word + '\\t' + doc +'\\t' + str(index_words[word][doc]) +'\\t' + str(idf[word]) + '\\t' + str(tfidf) + '\\n')\n",
    "\n",
    "# ファイルを閉じる\n",
    "f.close()\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題1-2：索引付けプログラムの説明文を書きなさい\n",
    "\n",
    "- 課題1-1のプログラムを詳細に説明したものです。\n",
    "\n",
    "> 6: ディレクトリ内にあるファイルを読み込むための`os`ライブラリを読み込む\n",
    "\n",
    "> 7: 分かち書きをするライブラリ`janome.tokenizer`から、分かち書きクラス`Tokenizer`を読み込む\n",
    "\n",
    "> 8: 正規表現をあつかう`re`パッケージを読み込む\n",
    "\n",
    "> 9: 数学処理を扱う`math`ライブラリをインポート\n",
    "\n",
    "> 12: 分かち書きクラス`Tokenizer`を初期化し、クラスのショートカットを変数`t`に格納する\n",
    "\n",
    "> 17: CJE3の`data`フォルダを指定\n",
    "\n",
    "> 19: CJE3の`index`フォルダを指定\n",
    "\n",
    "> 20: 出力ファイルのフォルダとファイル名を指定\n",
    "\n",
    "> 25: 索引語用辞書オブジェクト`index_words`を初期化\n",
    "\n",
    "> 27: 不要語辞書`stopworrds`を初期化\n",
    "\n",
    "> 29: ファイル名保存用辞書オブジェクト`docs`を初期化\n",
    "\n",
    "> 31: df用辞書オブジェクト`df`の初期化\n",
    "\n",
    "> 33: idf用辞書オブジェクト`idf`の初期化\n",
    "\n",
    "> 40: 不要語として削除したい文字列のパターンを定義し, 変数`pattern`に保存（今回はひらがな1文字のみのものと句読点）\n",
    "\n",
    "> 43~44: 不要語として削除したい文字列をキー, 値を1として`stopwords`にレコードを追加\n",
    "\n",
    "> 51: `os`ライブラリの`listdir`メソッドを使って、変数DATAが参照するフォルダに含まれるファイル名のリストを取得。最初のファイル名を変数`filename`に格納。forメソッドを使って以下の処理をリストの終わりまで繰り返す\n",
    "\n",
    "> 53: 変数`filename`を`open`メソッドを使って読み出し専用モード（'r'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数`f`に格納する\n",
    "\n",
    "> 55: 変数`f`の内容を1行ずつ取り出し変数`line`に格納する。これを最終行まで繰り返す。\n",
    "\n",
    "> 57: 変数lineに格納された行を、`tokenize`メソッドで分かち書きして、その結果を変数`tokens`に格納\n",
    "\n",
    "> 59: 変数`tokens`に格納された要素を1つずつ取り出し変数`token`に格納する。これを最後の要素になるまで繰り返す。\n",
    "\n",
    "> 62: `surface`メソッドを使って得られる単語本体が`pattern`にマッチした場合の処理\n",
    "\n",
    "> 63: それ以降の処理をスキップする\n",
    "\n",
    "> 66: `surface`メソッドを使って得られる単語本体をキーとするレコードが`stopwords`に存在していた場合の処理\n",
    "\n",
    "> 67: それ以降の処理をスキップする\n",
    "\n",
    "> 70: `surface`メソッドを使って得られる単語本体ををキーとするレコードが`index_words`に存在していた場合の処理\n",
    "\n",
    "> 72: もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していた場合の処理\n",
    "\n",
    "> 73: 語とファイル名をキーとするレコードの値を1増やす\n",
    "\n",
    "> 76: 72行目が`false`だった場合\n",
    "\n",
    "> 77: 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "> 80: 70行目が`false`だった場合\n",
    "\n",
    "> 81: そのキーを使って`index_words`に無名辞書オブジェクトを初期化\n",
    "\n",
    "> 82: 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "> 92: `index_words`のキーを変数`word`として1つずつ取り出し最後の要素まで繰り返す\n",
    "\n",
    "> 94: 索引語キーが指す入れ子オブジェクト内のキーを変数`doc`として取り出しを一つずつ処理する\n",
    "\n",
    "> 96: `not in`メソッドを使って、変数`doc`がもつファイル名が、辞書オブジェクト`docs`に存在しない時の処理\n",
    "\n",
    "> 97: ファイル名をキー, 値を1としたレコードを`docs`に追加する\n",
    "\n",
    "> 100: `len`メソッドを使って`docs`の要素数を取得し, 変数`docs_size`に総文書数として保存する\n",
    "\n",
    "> 105: `index_words`のキーを変数`word`として1つずつ取り出し、最後の要素まで繰り返す\n",
    "\n",
    "> 107: `len`メソッドを使って`word`が指す入れ子辞書の要素数を取得し, `df`に代入する\n",
    "\n",
    "> 112: `df`のキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 114: idf値を定義に基づいて計算する\n",
    "\n",
    "> 121: 変数`index_file`を`open`メソッドを使って書き出し専用モード（'w'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数`f2`に格納する\n",
    "\n",
    "> 124: `index_words`のソートされたキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 126: そのキーが指す入れ子オブジェクト内のソートされたキーを変数`doc`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 127: tfidf値を定義に基づいて計算する\n",
    "\n",
    "> 130: `write`メソッドを使って索引語と重みデータをファイルに書き出す\n",
    "\n",
    "> 133~134: 開いたファイルを`close`メソッドを使って閉じる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題2-1：順位付け処理のプログラムを書きなさい\n",
    "\n",
    "- ノートブック5で作成したプログラムを清書したものです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------\n",
    "# 1. 初期設定\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. ライブラリの読み込み\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# 分かち書きクラスの初期化\n",
    "t = Tokenizer()\n",
    "\n",
    "### B. フォルダやファイル名の設定\n",
    "\n",
    "# 索引ファイルの指定\n",
    "INDEX = \"../index\"\n",
    "index_file = INDEX + \"/index3.txt\"\n",
    "\n",
    "### C. 各種辞書オブジェクトの初期化\n",
    "\n",
    "# 索引語idf値の辞書オブジェクト\n",
    "idf_scores = {}\n",
    "# 索引語tfidf値の辞書オブジェクト\n",
    "tfidf_scores = {}\n",
    "# 検索語tf値の辞書オブジェクト\n",
    "query_tf = {}\n",
    "# 検索語tfidf値の辞書オブジェクト\n",
    "query_tfidf = {}\n",
    "# 不要語辞書オブジェクト\n",
    "stopwords = {}\n",
    "# 検索語の辞書オブジェクト\n",
    "query_words = {}\n",
    "# 順位付け対象文書用辞書オブジェクト\n",
    "ranking_docs = {}\n",
    "\n",
    "#---------------------------------------------\n",
    "# 2. 索引ファイルの読み込み\n",
    "#---------------------------------------------\n",
    "\n",
    "# 索引ファイルを読み込みモードで開く\n",
    "f = open(index_file, 'r')\n",
    "\n",
    "# 索引ファイルを1行づつ処理\n",
    "for line in f:\n",
    "    ### A. 行末処理と要素の分割\n",
    "\n",
    "    # 行末の改行文字除去\n",
    "    line = line.rstrip()\n",
    "    # 行の分割\n",
    "    split_line = line.split(\"\\t\")\n",
    "    \n",
    "    # 配列要素の取得\n",
    "    word = split_line[0] \n",
    "    doc = split_line[1] \n",
    "    idf = float(split_line[3])\n",
    "    tfidf = float(split_line[4])\n",
    "\n",
    "    \n",
    "    ### B. 重み値の取得\n",
    "    idf_scores[word] = idf\n",
    "    # 辞書オブジェクトへの代入（改良版）\n",
    "    if word in tfidf_scores:\n",
    "        tfidf_scores[word][doc] = tfidf\n",
    "        \n",
    "    else:\n",
    "        tfidf_scores[word] = {}\n",
    "        tfidf_scores[word][doc] = tfidf\n",
    "\n",
    "        \n",
    "### C. tfidf_scoresからデータフレームを作成\n",
    "tfidf_table = pd.DataFrame(tfidf_scores)\n",
    "\n",
    "# NaNを0に置き換える\n",
    "tfidf_table = tfidf_table.fillna(0)\n",
    "\n",
    "#---------------------------------------------\n",
    "# 3. 検索質問の処理\n",
    "#---------------------------------------------\n",
    "\n",
    "# 検索質問\n",
    "query = '吾輩は猫である'\n",
    "# 検索質問データフレーム用ファイル名\n",
    "query_file = 'query'\n",
    "\n",
    "### A. 不用語削除ルールの定義\n",
    "\n",
    "# 不要語としてマッチしたいパターンの定義\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "# 不要語の追加\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "### B. 検索質問の分かち書き\n",
    "tokens = t.tokenize(query)\n",
    "\n",
    "# 分かち書きされた語オブジェクトの処理\n",
    "for token in tokens:\n",
    "    ### C. 不用語処理\n",
    "    # 正規表現\n",
    "    if pattern.match(token.surface):\n",
    "        continue\n",
    "\n",
    "    # 不用語リスト\n",
    "    if token.surface in stopwords:\n",
    "        continue\n",
    "\n",
    "    #  検索語の追加とtf値の加算\n",
    "    if token.surface in query_words:\n",
    "        query_words[token.surface] += 1\n",
    "    else:\n",
    "        query_words[token.surface] = 1\n",
    "\n",
    "### D. 検索語の重み付け\n",
    "# 索引語を一つずつ処理\n",
    "for word in idf_scores:\n",
    "    # 索引語をキーとした検索質問tf値用とtfidf値用オブジェクトを初期化\n",
    "    query_tf[word] = {}  \n",
    "    # キー：索引語、擬似文書名「query」、値：0\n",
    "    query_tf[word][query_file] = 0   \n",
    "\n",
    "# 検索語を一つずつ処理\n",
    "for query_word in query_words:   \n",
    "    # 検索語==索引語なレコードのtf値をquery_wordsから代入\n",
    "    for index_word in idf_scores:\n",
    "        if query_word == index_word:\n",
    "            query_tf[query_word][query_file] = query_words[query_word]\n",
    "\n",
    "# 検索語の出現頻度（`tf`値）と`idf`値を使って、`tfidf`値を算出する\n",
    "for word in query_tf:\n",
    "    for q_value in query_tf[word]:\n",
    "        tf = query_tf[word][q_value]\n",
    "        idf = idf_scores[word]\n",
    "        tfidf = tf * idf\n",
    "        query_tfidf[word] = tfidf\n",
    "\n",
    "### D. query_tfidfからデータフレームを作成\n",
    "query_table = pd.DataFrame(query_tfidf, index=['query',])\n",
    "\n",
    "#---------------------------------------------\n",
    "# 4. 類似度の計算と順位付け\n",
    "#---------------------------------------------\n",
    "\n",
    "### A. 順位付け対象文書の同定\n",
    "for query_word in query_words:\n",
    "    for doc in tfidf_scores[query_word]:\n",
    "        # 値`1`で初期化\n",
    "        ranking_docs[doc] = 1\n",
    "\n",
    "# 対象文書を一つずつ処理\n",
    "for doc in ranking_docs:\n",
    "    ### B. 余弦関数の分子の算出\n",
    "    \n",
    "    # 分子変数の初期化\n",
    "    numerator = 0\n",
    "    # 文書データフレームからdocにマッチする行のデータを取得\n",
    "    doc_vec = tfidf_table.loc[doc]\n",
    "    # 検索質問データフレームからquery_fileにマッチする行のデータを取得\n",
    "    query_vec = query_table.loc['query']\n",
    "    # 検索質問ベクトルの要素を添字を使って巡回\n",
    "    for i in range(len(query_vec.values)):\n",
    "        # query_vecとdoc_vecのi番目の要素を掛け合わせて、分子変数に足していく\n",
    "        i_value = query_vec.values[i] * doc_vec.values[i]\n",
    "        numerator += i_value\n",
    "\n",
    "    ### C. 余弦関数の分母の算出\n",
    "    \n",
    "    # 分母変数の初期化\n",
    "    denominator = 0\n",
    "    # 検索語ベクトル積の変数の初期化\n",
    "    query_value = 0\n",
    "    # 索引語ベクトル積の変数の初期化\n",
    "    doc_value = 0\n",
    "    # 検索質問ベクトルの要素を添字を使って巡回\n",
    "    for i in range(len(query_vec.values)):\n",
    "        # 検索質問ベクトルのi番目の値の二乗をquery_valueに加算していく\n",
    "        query_value += query_vec.values[i] ** 2\n",
    "        # 文書ベクトルのi番目の値の二乗をdoc_valueに加算していく\n",
    "        doc_value += doc_vec.values[i] ** 2\n",
    "    \n",
    "    # query_valueとdoc_valueの平方根を掛け合わせて分母とする\n",
    "    denominator = math.sqrt(query_value) * math.sqrt(doc_value)\n",
    "    \n",
    "    ### D. 余弦関数の算出\n",
    "    cosine = numerator / denominator\n",
    "    \n",
    "    # ranking_docsへ値を代入\n",
    "    ranking_docs[doc] = cosine\n",
    "\n",
    "#---------------------------------------------\n",
    "# 5. 順位付け結果の出力\n",
    "#---------------------------------------------\n",
    "\n",
    "# 類似度の高い順に文書を表示\n",
    "sorted(ranking_docs.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題2-2：順位付けプログラムの説明文を書きなさい\n",
    "\n",
    "- 課題2-1のプログラムを詳細に説明したものです。\n",
    "\n",
    "> 6: ディレクトリ内にあるファイルを読み込むための`os`ライブラリを読み込む\n",
    "\n",
    "> 7: 分かち書きをするライブラリ`janome.tokenizer`から、分かち書きクラス`Tokenizer`を読み込む\n",
    "\n",
    "> 8: 正規表現をあつかう`re`パッケージを読み込む\n",
    "\n",
    "> 9: 数学処理を扱う`math`ライブラリをインポート\n",
    "\n",
    "> 10: データ分析ライブラリの`pandas`を`pd`としてインポート\n",
    "\n",
    "> 13: 分かち書きクラス`Tokenizer`を初期化し、クラスのショートカットを変数`t`に格納する\n",
    "\n",
    "> 18: CJE3の`index`フォルダを指定\n",
    "\n",
    "> 19: 出力ファイルのフォルダとファイル名を指定\n",
    "\n",
    "> 24: 索引語idf値用辞書オブジェクト`idf_scores`を初期化\n",
    "\n",
    "> 26: 索引語tfidf値用辞書オブジェクト`tfidf_scores`を初期化\n",
    "\n",
    "> 28: 検索語tf値用辞書オブジェクト`query_tf`を初期化\n",
    "\n",
    "> 30: 検索語tfidf値用辞書オブジェクト`query_tfidf`の初期化\n",
    "\n",
    "> 32: 不要語辞書オブジェクト`stopworrds`を初期化\n",
    "\n",
    "> 34: 検索語辞書オブジェクト`query_words`を初期化\n",
    "\n",
    "> 36: 順位付け対象文書用辞書オブジェクト`ranking_docs`を初期化\n",
    "\n",
    "> 43: 変数`index_file`を`open`メソッドを使って読み出し専用モード（'r'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数`f`に格納する\n",
    "\n",
    "> 46: 変数`f`の内容を1行ずつ取り出し変数`line`に格納する。これを最終行まで繰り返す。\n",
    "\n",
    "> 50: `rstrip`メソッドを使って`line`に含まれている行末の改行文字を削除\n",
    "\n",
    "> 52: `split`メソッドを使ってlineを`\\t`ごとに区切りそれを配列`split_words`に保存\n",
    "\n",
    "> 55~58: `split_line`の要素を取得しそれぞれ`word`, `doc`, `idf`, `tfidf`として保存。その際`idf`と`tfidf`はfloat型でキャストする\n",
    "\n",
    "> 62: `word`をキー, `idf`を値とするレコードを`idf_scores`に追加\n",
    "\n",
    "> 64: `tfidf_scores`に`word`をキーとするレコードが存在していた場合の処理\n",
    "\n",
    "> 65: `word`と`doc`をキー, `tfidf`を値とするレコードを`tfidf_scores`に追加する\n",
    "\n",
    "> 67: 64行目が`false`だった場合の処理\n",
    "\n",
    "> 68: `word`をキーとして`tfidf_scores`に無名辞書オブジェクトを初期化\n",
    "\n",
    "> 69: `word`と`doc`をキー, `tfidf`を値とするレコードを`tfidf_scores`に追加する\n",
    "\n",
    "> 73: `tfidf_scores`を、`pd.DataFrame`メソッドを用いてデータフレームに変換し、変数`tfidf_table`に保存\n",
    "\n",
    "> 76: `fillna`メソッドを使ってNaNを0に置き換える\n",
    "\n",
    "> 83: `query`を定義\n",
    "\n",
    "> 85: 検索質問データフレーム用ファイル名を\"query\"として`query_file`に保存\n",
    "\n",
    "> 90: 不要語として削除したい文字列のパターンを定義し, 変数`pattern`に保存（今回はひらがな1文字のみのものと句読点）\n",
    "\n",
    "> 92~93: 不要語として削除したい文字列をキー, 値を1として`stopwords`にレコードを追加\n",
    "\n",
    "> 96: 変数`query`に格納された質問文を、`tokenize`メソッドで分かち書きして、その結果を変数`tokens`に格納\n",
    "\n",
    "> 99: 変数`tokens`に格納された要素を1つずつ取り出し変数`token`に格納する。これを最後の要素になるまで繰り返す。\n",
    "\n",
    "> 102: `surface`メソッドを使って得られる単語本体が`pattern`にマッチした場合の処理\n",
    "\n",
    "> 103: それ以降の処理をスキップする\n",
    "\n",
    "> 106: `surface`メソッドを使って得られる単語本体をキーとするレコードが`stopwords`に存在していた場合の処理\n",
    "\n",
    "> 107: それ以降の処理をスキップする\n",
    "\n",
    "> 110: `surface`メソッドを使って得られる単語本体をキーとするレコードが`query_words`に存在していた場合の処理\n",
    "\n",
    "> 111: tf値を1増やす\n",
    "\n",
    "> 112: 110行目が`false`だったときの処理\n",
    "\n",
    "> 113: `surface`メソッドを使って得られる単語本体をキー, 値を1とするレコードを`query_words`に登録\n",
    "\n",
    "> 117: `idf_scores`のキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 119: 索引語をキーとした無名辞書オブジェクトを初期化\n",
    "\n",
    "> 121: 索引語, 擬似文書名をキー, 値を0としてレコード登録\n",
    "\n",
    "> 124: `query_words`のキーを変数`query_word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 126: `idf_scores`のキーを変数`index_word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 127: `query_word`と`index_word`が一致した場合の処理\n",
    "\n",
    "> 128: そのキーのtf値を`query_words`から`query_tf`に代入\n",
    "\n",
    "> 131: `query_tf`のキーを変数`word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 132: `query_tf`の`word`をキーとするレコードを変数`q_value`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 133~136: tf値, idf値を取り出しそれを用いてtfidf値を算出して`query_tfidf`に`word`をキー, `tfidf`を値としてレコード登録\n",
    "\n",
    "> 139: `uery_tfidf`からデータフレームを作成\n",
    "\n",
    "> 146: `query_words`のキーを変数`query_word`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 147: `tfidf_scores`の`query_word`をキーとするレコードを変数`doc`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 148: `ranking_docs`に`doc`をキー, 値を1でレコード登録\n",
    "\n",
    "> 152: `ranking_docs`のキーを`doc`として1つずつ取り出し, 最後の要素まで繰り返す\n",
    "\n",
    "> 156: `numerator`の初期化\n",
    "\n",
    "> 158: `loc`メソッドを使って文書データフレームから`doc`にマッチする行のデータを取得し, `doc_vec`に保存\n",
    "\n",
    "> 160: `loc`メソッドを使って検索質問データフレームから`query_file`にマッチする行のデータを取得し, `query_vec`に保存\n",
    "\n",
    "> 162: `len`メソッドを使って`query_vec`の要素数を取得しその分だけ添字iを使って巡回する\n",
    "\n",
    "> 164~165: `query_vec`と`doc_vec`のi番目の要素を掛け合わせて、`numerator`に足していく\n",
    "\n",
    "> 170~174: 計算に使う変数を初期化しそれぞれ`denominator`, `query_value`, `doc_value`とする\n",
    "\n",
    "> 176: `len`メソッドを使って`query_vec`の要素数を取得しその分だけ添字iを使って巡回する\n",
    "\n",
    "> 178: `query_vec`のi番目の値の二乗を`query_value`に加算していく\n",
    "\n",
    "> 180: `doc_vec`のi番目の値の二乗を`doc_value`に加算していく\n",
    "\n",
    "> 183: `query_value`と`doc_value`の平方根を掛け合わせて分母とし, `denominator`に保存\n",
    "\n",
    "> 186: cosine類似度の定義に基づきそれを算出し`cosine`に保存\n",
    "\n",
    "> 189: `ranking_docs`の値を`cosine`で書き換える\n",
    "\n",
    "> 196: `sorted`メソッドに値でソートし降順で表示するオプションを追加し`ranking_docs`のレコードを表示する\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題3-1（任意）：順位付け処理プログラムを高度化しなさい\n",
    "\n",
    "- 課題2-1で作成した順位付け処理プログラムを高度化しなさい。高度化する内容は自由に設定して良い。適宜[日本語マニュアル](https://docs.python.jp/3.5/index.html)やその他のリソースを参照しなさい。\n",
    "- 高度化の例\n",
    "  - （関数を用いた）プログラムのモジュール化\n",
    "  - インタラクティブシステム（ユーザの入力（検索質問）を受け取って、整形した検索結果を返す、など）\n",
    "  - 異なる順位付けアルゴリズムの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(index_file):\n",
    "    f = open(index_file, 'r')\n",
    "    idf_scores = {}\n",
    "    tfidf_scores = {}\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        split_line = line.split(\"\\t\")\n",
    "        word = split_line[0]\n",
    "        doc = split_line[1]\n",
    "        idf = float(split_line[3])\n",
    "        tfidf = float(split_line[4])\n",
    "\n",
    "        idf_scores[word] = idf\n",
    "        if word in tfidf_scores:\n",
    "            tfidf_scores[word][doc] = tfidf\n",
    "        else:\n",
    "            tfidf_scores[word] = {}\n",
    "            tfidf_scores[word][doc] = tfidf\n",
    "    return idf_scores, tfidf_scores\n",
    "\n",
    "def deal_with_query(query, tfidf_scores):\n",
    "    stopwords = {}\n",
    "    query_words = {}\n",
    "    ranking_docs = {}\n",
    "    pattern = re.compile(r\"^[　-ー]$\")\n",
    "    stopwords['という'] = 1\n",
    "    stopwords['にて'] = 1\n",
    "    tokens = t.tokenize(query)\n",
    "    for token in tokens:\n",
    "        if pattern.match(token.surface):\n",
    "            continue\n",
    "        if token.surface in stopwords:\n",
    "            continue\n",
    "        if token.surface in query_words:\n",
    "            query_words[token.surface] += 1\n",
    "        else:\n",
    "            query_words[token.surface] = 1\n",
    "    return query_words, ranking_docs\n",
    "\n",
    "def query_weighting(idf_scores, query_file, query_words):\n",
    "    query_tf = {}\n",
    "    query_tfidf = {}\n",
    "    for index_word in idf_scores:\n",
    "        query_tf[index_word] = {}\n",
    "        query_tf[index_word][query_file] = 0\n",
    "    for query_word in query_words:\n",
    "        for index_word in idf_scores:\n",
    "            if query_word == index_word:\n",
    "                query_tf[query_word][query_file] = query_words[query_word]\n",
    "    for query_word in query_tf:\n",
    "        for doc in query_tf[query_word]:\n",
    "            tf = query_tf[query_word][doc]\n",
    "            idf = idf_scores[query_word]\n",
    "            tfidf = tf * idf\n",
    "            query_tfidf[query_word] = tfidf\n",
    "    return query_tfidf\n",
    "\n",
    "def create_dataframe(tfidf_scores, query_tfidf):\n",
    "    tfidf_table = pd.DataFrame(tfidf_scores)\n",
    "    tfidf_table = tfidf_table.fillna(0)\n",
    "    query_table = pd.DataFrame(query_tfidf, index=['query',])\n",
    "    return tfidf_table, query_table\n",
    "\n",
    "def identify_docs(query_words, tfidf_scores, ranking_docs):\n",
    "    try:\n",
    "        for query_word in query_words:\n",
    "            for doc in tfidf_scores[query_word]:\n",
    "                ranking_docs[doc] = 1\n",
    "    except KeyError:\n",
    "        print(\"Your query was void.\")\n",
    "        exit()\n",
    "    return ranking_docs\n",
    "\n",
    "def cosine_similality(ranking_docs, tfidf_table, query_table):\n",
    "    for doc in ranking_docs:\n",
    "        doc_vec = tfidf_table.loc[doc]\n",
    "        query_vec = query_table.loc['query']\n",
    "        numerator = np.matmul(doc_vec.values, query_vec.values)\n",
    "        denominator = np.linalg.norm(query_vec.values) * np.linalg.norm(doc_vec.values)\n",
    "        cosine = numerator / denominator\n",
    "        ranking_docs[doc] = cosine\n",
    "    return ranking_docs\n",
    "\n",
    "def output(ranking_docs):\n",
    "    print(sorted(ranking_docs.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "def do_func():\n",
    "    idf_scores = read_file(index_file)[0]\n",
    "    tfidf_scores = read_file(index_file)[1]\n",
    "    query_words = deal_with_query(query, tfidf_scores)[0]\n",
    "    if not query_words:\n",
    "        print(\"Please input not a letter but words or sentences\")\n",
    "        exit()\n",
    "    ranking_docs = deal_with_query(query, tfidf_scores)[1]\n",
    "    query_tfidf = query_weighting(idf_scores, query_file, query_words)\n",
    "    tfidf_table = create_dataframe(tfidf_scores, query_tfidf)[0]\n",
    "    query_table = create_dataframe(tfidf_scores, query_tfidf)[1]\n",
    "    ranking_docs = identify_docs(query_words, tfidf_scores, ranking_docs)\n",
    "    ranking_docs = cosine_similality(ranking_docs, tfidf_table, query_table)\n",
    "    output(ranking_docs)\n",
    "\n",
    "#---------------------------------------------\n",
    "# プログラム本体\n",
    "#---------------------------------------------\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "t = Tokenizer()\n",
    "\n",
    "INDEX = \"../index\"\n",
    "index_file = INDEX + \"/index3.txt\"\n",
    "query = \"\"\n",
    "while True:\n",
    "    input_words = str(input(\"Query ->\"))\n",
    "    if not input_words:\n",
    "        break\n",
    "    else:\n",
    "        query += \"\".join(input_words)\n",
    "\n",
    "query_file = 'query'\n",
    "do_func()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題3-2（任意）：高度化プログラムの説明文を書きなさい\n",
    "\n",
    "- 課題3-1のプログラムを詳細に説明したものです。特に高度化した内容について丁寧に説明すること。\n",
    "\n",
    "- 関数を用いたモジュール化, ユーザの入力を受け取るインタラクティブシステム, 異なる順位付けアルゴリズムを実装した\n",
    "\n",
    "- 2-1で実装し2-2で説明済みのアルゴリズムに関しては冗長になるため記述しない\n",
    "\n",
    "> 1~19: 索引ファイルを読み込み重みの値を取得する関数 ( 引数:`index_file`, 戻り値: `idf_scores`, `tfidf_scores` )\n",
    "\n",
    "> 21~38: 検索質問を処理する関数( 引数:`query`, `tfidf_socres`, 戻り値: `query_words`, `ranking_docs` )\n",
    "\n",
    "> 40~56: 検索語の重み付けを行う関数( 引数:`idf_scores`, `query_file`, `query_words`, 戻り値: `query_tfidf` )\n",
    "\n",
    "> 58~62: データフレームを作成する関数( 引数:`tfidf_scores`, `query_tfidf`, 戻り値: `tfidf_table`, `query_table` )\n",
    "\n",
    "> 64~72: 対象文書を同定する関数( 引数:`query_words`, `tfidf_scores`, `ranking_docs`, 戻り値: `ranking_docs` )\n",
    "\n",
    "> 69~71: 検索語がどの文書にも含まれていなかった場合の例外処理\n",
    "\n",
    "> 74~82: cosine類似度を計算する関数( 引数:`ranking_docs`, `tfidf_table`, `query_table`, 戻り値: `ranking_docs` )\n",
    "\n",
    "> 78: `np.matmul`メソッドを用いて`doc_vec`と`query_vec`の内積を計算し`numerator`に保存\n",
    "\n",
    "> 79: `np.linalg.norm`メソッドを用いて`doc_vec`と`query_vec`のノルムを計算し`denominator`に保存\n",
    "\n",
    "> 84~85: 結果を出力する関数( 引数:`ranking_docs`, 戻り値: なし )\n",
    "\n",
    "> 87~100: これまでに作成した関数を呼び出し戻り値を受け取る関数( 引数: なし, 戻り値: なし)\n",
    "\n",
    "> 88: `read_file`を呼び出し,その0番目の戻り値を変数`idf_scores`に保存\n",
    "\n",
    "> 89: `read_file`を呼び出し,その1番目の戻り値を変数`tfidf_scores`に保存\n",
    "\n",
    "> 90: `deal_with_query`を呼び出し,その0番目の戻り値を変数`query_words`に保存\n",
    "\n",
    "> 91~93: 不要語としてクエリが全て削除されてしまった場合の処理\n",
    "\n",
    "> 94: `deal_with_query`を呼び出し,その1番目の戻り値を変数`ranking_docs`に保存\n",
    "\n",
    "> 95: `query_weighting`を呼び出し,その戻り値を変数`query_tfidf`に保存\n",
    "\n",
    "> 96: `create_dataframe`を呼び出し,その0番目の戻り値を変数`tfidf_table`に保存\n",
    "\n",
    "> 97: `create_dataframe`を呼び出し,その1番目の戻り値を変数`query_table`に保存\n",
    "\n",
    "> 98: `identify_docs`を呼び出し,その戻り値を変数`ranking_docs`に保存\n",
    "\n",
    "> 99: `cosine_similality`を呼び出し,その戻り値に変数`ranking_docs`を書き換え\n",
    "\n",
    "> 100: `output`の呼び出し\n",
    "\n",
    "> 109: 数値計算をおこなう`numpy`モジュールを`np`としてインポート\n",
    "\n",
    "> 115: 変数`query`の初期化\n",
    "\n",
    "> 116: `True`の間繰り返す\n",
    "\n",
    "> 117: ユーザからの入力を変数`input_words`に文字列型で保存\n",
    "\n",
    "> 118: `input_words`に何も入力されなかった場合の処理\n",
    "\n",
    "> 119: 無限ループを抜け出す\n",
    "\n",
    "> 120: 118行目が`false`だった場合の処理\n",
    "\n",
    "> 121: 変数`query`に`input_words`を付け足す（複数行の入力に対応）\n",
    "\n",
    "> 123: 擬似文書名を`query_file`に保存\n",
    "\n",
    "> 124: `do_func`の呼び出し\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 課題4（任意）：後半5回の演習の感想を書いてください。\n",
    "\n",
    "\n",
    "##### 演習で初めて（あるいは改めて）学んだこと\n",
    "- 秋ABの授業で自然言語処理の講義を履修したが, そこでは理論がメインで手を動かして実装する時間は少なかったので, この演習を通じてその理論を復習することができたと思う。\n",
    "\n",
    "- pythonの基本的な文法のいい復習になったほか, 入れ子辞書を使って処理をする経験は初めてだったのでいい経験になった。\n",
    "\n",
    "- markdownの書き方を少しだけ覚えることができた。\n",
    "\n",
    "##### 演習の優れていた点\n",
    "- 演習用ノートブックの解説が丁寧だった。必要な処理を逐一コメントアウトで提示してもらえていたので入れ子辞書の処理などとっつきにくいアルゴリズムも躓くことなくコーディングできた。\n",
    "\n",
    "##### 演習の改善点\n",
    "- 各演習少ないものでも30行, 多いと100行以上あるコードについて1行ずつ説明するのは骨の折れる作業なので別の方法を探したほうがいいと思います。\n",
    "\n",
    "\n",
    "##### その他の感想やメッセージ\n",
    "- ありがとうございました！\n",
    "\n",
    "※課題4の内容は成績に影響しません。次年度演習の参考にさせていただきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright &copy; 2019. Hideo Joho and Haitao Yu. All rights reserved.  \n",
    "無断複製・転載・配布行為を禁止します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
