{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**学生用**\n",
    "\n",
    "# 2019年度知識情報演習III（後半）ノートブック2\n",
    "本ノートブックでは、情報検索システムにおける索引付け処理の後半部分の手続きを理解しながら、Pythonでプログラムを作成していきます。\n",
    "\n",
    "## 完了の目安\n",
    "本ノートブックは、後半第2回（今回）の終わりか、第3回（次回）の前半までに完了することを目安にしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ノートブック1の演習内容をTAか教員に確認してもらってから、ノートブック2に進んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 【重要】本演習の学習方針\n",
    "\n",
    "プログラミングでは、ある問題を解決するための方法が複数存在するのが普通です。しかし本演習では、Pythonを使ったプログラミングにおいて**標準的な実装方法**を身につけることを重視します。したがって、以下の点に注意して演習を進めてください。\n",
    "\n",
    "- プログラムを作成する際に、ノートブックで説明されたやり方に沿って実装すること\n",
    "  - TAや教員もノートブックで説明した実装方法が実践できているかを確認します\n",
    "- 外部ウェブサイトなどで示された別の実装方法を参照した場合、プログラムが正常に機能したとしても**やり直しをしてもらう場合がある**\n",
    "- 情報検索の仕組みを理解するために、便利なライブラリを意図的に使わない場合がある\n",
    "- 情報検索システムの理解に直接関係ない複雑な処理に関して、便利なライブラリを使用する場合がある\n",
    "- その他の関連情報については、参照URLを示すので、授業外活動として各自学習しておくこと"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 【重要】本演習の実行環境\n",
    "\n",
    "プログラミングは、プログラムを実行する環境によって動作や出力結果が異なる場合があります。全学計算機システムの実行環境は以下の通りです。\n",
    "\n",
    "- OS : Ubuntu 16.04 LTS\n",
    "- Python : Anaconda Python 3 ver 5.2.0\n",
    "- JupyterLab : 0.32.1\n",
    "- Janome : 0.36\n",
    "\n",
    "研究室や個人のパソコンを使った場合、上記の実行環境と異なると同じ結果が出ない場合やエラーが出る（あるいは出ない）場合がありますので、注意が必要です。本演習では上記実行環境以外の動作に関するサポートは行いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 索引付け Part II\n",
    "\n",
    "索引付け処理の後半では、以下の手続きを順に行っていきます。\n",
    "\n",
    "> 1. ファイル単位で単語の出現頻度を数える  \n",
    "> 2. 索引語をあいうえお順に並べる  \n",
    "> 3. 不要語の削除  \n",
    "> 4. 索引ファイルの保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 演習用フォルダ構成の確認\n",
    "作業を始める前に、演習用のフォルダが以下の構成になっていることを確認してください。  \n",
    "\n",
    "**注意：** 今回は新しく`index`というフォルダを`CJE3`の下に作成しておいてください。ここに作成する索引ファイルを保存します。\n",
    "\n",
    "```\n",
    "ホームフォルダ\n",
    "  ┗ CJE3\n",
    "    ┗ ipynb\n",
    "      ┣ CJE3_2019_Notebook_0.ipynb\n",
    "      ┣ CJE3_2019_Notebook_1.ipynb\n",
    "      ┣ CJE3_2019_Notebook_2.ipynb（新規）\n",
    "      ┗ images（新規）\n",
    "          ┗ nested_dict.png（新規）\n",
    "    ┗ data\n",
    "      ┣ sample_doc1.txt\n",
    "      ┣ ...\n",
    "      ┗ sample_doc10.txt\n",
    "    ┗ index (新規）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ファイル単位で単語の出現頻度を数える\n",
    "それでは、索引付けの後半作業を始めましょう。\n",
    "\n",
    "ここまでのプログラムで、複数ファイルにアクセスして、行を分かち書きし、単語ごとの出現回数を数えてきました。\n",
    "\n",
    "このような処理を情報検索システムに応用するには、もう一工夫必要になります。それは、単語の出現回数を**ファイル単位で数える**ことです。何故ならば、検索エンジンは最終的にウェブページをランキングすることが目的なので、単語の統計情報もページ（ファイル）単位で計算しておく必要があるからです。\n",
    "\n",
    "これまでの処理とこの先の処理を比べると以下のようになります。\n",
    "\n",
    "> これまで\n",
    "\n",
    "| 単語 | 出現回数 |\n",
    "| :--: | :--: |\n",
    "| 我輩 | 2 |\n",
    "| 人間 | 2 |\n",
    "\n",
    "> この先\n",
    "\n",
    "| 単語 | ファイル | 出現回数 |\n",
    "| :--: | :--: | :--: |\n",
    "| 我輩 | sample_doc1.txt | 2 |\n",
    "| 人間 | sample_doc1.txt | 1 |\n",
    "| 人間 | sample_doc2.txt | 1 |\n",
    "\n",
    "\n",
    "ご覧の通り、これまでのプログラムでは、**我輩**と**人間**の出現回数が2ということしか分かりませんでしたが、この後に作成するプログラムでは、**我輩**は1つのファイルに2回出現し、**人間**は2つのファイルに1回ずつ出現していることがわかるようになります。\n",
    "\n",
    "このような構造のデータを作成することで、本の後ろにある索引に近づいていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 辞書オブジェクトで複数のキーをつかう\n",
    "\n",
    "さて、上の表のようなデータ構造を作成するにはどうすればよいのでしょうか？\n",
    "\n",
    "実装方法はいくつか存在するのですが、本演習では**辞書オブジェクトで複数のキーを使う**手法を採用したいと思います。\n",
    "\n",
    "- これまで：`キー`＝単語、`値`=出現頻度\n",
    "- このさき：`キー1`=単語、`キー2`=ファイル名、`値`=出現頻度\n",
    "\n",
    "という設計です。このようにすることで、ある単語があるファイルに出現する回数を値として保持できます。\n",
    "\n",
    "それでは、辞書オブジェクトで複数のキーと値で構成されるレコードを扱う例を見てましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# キーを二つ使った辞書オブジェクトの生成\n",
    "dict = {}\n",
    "dict['キー1'] = {}\n",
    "dict['キー1']['キー2'] = '値'\n",
    "#　値の出力\n",
    "print(dict['キー1']['キー2'])\n",
    "\n",
    "dict[\"a\"] = {}\n",
    "dict[\"a\"][\"b\"] = 1\n",
    "if \"b\" in dict[\"a\"]:\n",
    "    dict[\"a\"][\"b\"] += 1\n",
    "else:\n",
    "    dict[\"a\"][\"b\"] = {}\n",
    "\n",
    "print(dict[\"a\"][\"b\"])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムは、\n",
    "\n",
    "> 2: 辞書オブジェクト`dict`の初期化  \n",
    "> 3: 1つ目の**キー1**の値に、**無名の**辞書オブジェクトを初期化して追加  \n",
    "> 4: 1つ目のキー1と2つ目のキー2が参照する値を格納  \n",
    "> 6: 1つ目のキー1と2つ目のキー2が参照する値を出力\n",
    "\n",
    "という処理をしています。\n",
    "\n",
    "#### 【重要】オブジェクトの入れ子と初期化\n",
    "\n",
    "2行目と3行目で、**辞書オブジェクトの初期化を2回**しているのがポイントです。また、同じ辞書オブジェクトを2回初期化しているように見えますが、**違います**。1回目は`dict`辞書オブジェクトの初期化で、2回目は`dict`辞書オブジェクトの値に挿入する別の`無名`辞書オブジェクトの初期化です。つまり、2つのキーを使って値を参照するという構造は、オブジェクトの中にもう一つ別のオブジェクトを生成することで実現できるのです。\n",
    "\n",
    "> このようにオブジェクトの中に別のオブジェクトをいれることを、**入れ子にする**または**ネストする**といいます。 この概念は複雑なデータを処理するための**非常に重要**な手法です。今後のプログラムでも頻繁に使用しますので、しっかりと理解しましょう。\n",
    "\n",
    "#### ネストした辞書オブジェクトのイメージ\n",
    "![入れ子のイメージ図](images/nested_dict.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ファイル単位で単語の出現頻度を数える（つづき）\n",
    "\n",
    "それでは、ネストした辞書オブジェクトを使って、単語の出現回数をファイル単位で数えるプログラムを書いてみましょう。\n",
    "\n",
    "辞書オブジェクトの構造は\n",
    "\n",
    "- キー1：単語\n",
    "- キー2：ファイル名\n",
    "- 値：出現回数\n",
    "\n",
    "です。\n",
    "\n",
    "分かち書きの出力をネストした辞書オブジェクトに挿入していく方針は以下の通りです。\n",
    "\n",
    "1. もし`dict`に単語をキーとするレコードが存在していれば\n",
    "  1. もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "    1. 単語とファイル名をキーとするレコードの値を1増やす\n",
    "  1. さもなくば\n",
    "    1. 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "1. さもなくば\n",
    "  1. そのキーを使って`dict`に無名辞書オブジェクトを初期化\n",
    "  1. 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "\n",
    "**ヒント1：** `tokens`配列から`for`文をつかって変数`token`を一つずつ処理していく中で、`if-else`文を2段階（入れ子）にします。1段階目の`if-else`文では、単語をキーとするレコードが辞書オブジェクトに存在するかを確認し、2段階目の`if-else`文では、その単語をキーとするレコード（実体は無名辞書オブジェクト）内に、ファイル名をキーとするレコードが存在するかを確認します。\n",
    "\n",
    "**ヒント2：** 全体の出力のところで、`for`文をネストさせていますので、辞書オブジェクトのキーの扱い方を参考にしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "、\tsample_doc10.txt 5\n",
      "、\tsample_doc6.txt 2\n",
      "、\tsample_doc7.txt 1\n",
      "、\tsample_doc8.txt 1\n",
      "、\tsample_doc9.txt 3\n",
      "。\tsample_doc1.txt 5\n",
      "。\tsample_doc10.txt 6\n",
      "。\tsample_doc2.txt 3\n",
      "。\tsample_doc3.txt 3\n",
      "。\tsample_doc4.txt 3\n",
      "。\tsample_doc5.txt 3\n",
      "。\tsample_doc6.txt 5\n",
      "。\tsample_doc7.txt 7\n",
      "。\tsample_doc8.txt 5\n",
      "。\tsample_doc9.txt 7\n",
      "あっ\tsample_doc2.txt 1\n",
      "あっ\tsample_doc3.txt 1\n",
      "あっ\tsample_doc4.txt 1\n",
      "あと\tsample_doc2.txt 1\n",
      "あと\tsample_doc6.txt 1\n",
      "あまりに\tsample_doc4.txt 1\n",
      "ある\tsample_doc1.txt 1\n",
      "ある\tsample_doc10.txt 1\n",
      "ある\tsample_doc2.txt 1\n",
      "ある\tsample_doc3.txt 1\n",
      "ある\tsample_doc5.txt 1\n",
      "ある\tsample_doc7.txt 1\n",
      "ある\tsample_doc8.txt 1\n",
      "ある\tsample_doc9.txt 2\n",
      "あるい\tsample_doc10.txt 1\n",
      "あろ\tsample_doc3.txt 1\n",
      "い\tsample_doc10.txt 1\n",
      "い\tsample_doc7.txt 2\n",
      "いか\tsample_doc10.txt 1\n",
      "いくら\tsample_doc6.txt 1\n",
      "いた事\tsample_doc1.txt 1\n",
      "いっ\tsample_doc10.txt 1\n",
      "いっ\tsample_doc9.txt 1\n",
      "いとも\tsample_doc2.txt 1\n",
      "いる\tsample_doc1.txt 1\n",
      "いる\tsample_doc10.txt 1\n",
      "いる\tsample_doc3.txt 1\n",
      "いる\tsample_doc4.txt 1\n",
      "いる\tsample_doc6.txt 2\n",
      "いわゆる\tsample_doc3.txt 1\n",
      "う\tsample_doc3.txt 1\n",
      "う\tsample_doc6.txt 1\n",
      "う\tsample_doc8.txt 1\n",
      "うち\tsample_doc10.txt 1\n",
      "うち\tsample_doc6.txt 1\n",
      "おかしい\tsample_doc7.txt 1\n",
      "おっ\tsample_doc6.txt 1\n",
      "おっ\tsample_doc7.txt 1\n",
      "か\tsample_doc1.txt 1\n",
      "か\tsample_doc6.txt 2\n",
      "か\tsample_doc8.txt 1\n",
      "かかる\tsample_doc9.txt 1\n",
      "かた\tsample_doc4.txt 1\n",
      "かも\tsample_doc10.txt 1\n",
      "から\tsample_doc10.txt 1\n",
      "から\tsample_doc2.txt 1\n",
      "から\tsample_doc5.txt 1\n",
      "から\tsample_doc6.txt 1\n",
      "から\tsample_doc7.txt 1\n",
      "から\tsample_doc9.txt 2\n",
      "かん\tsample_doc4.txt 1\n",
      "が\tsample_doc1.txt 1\n",
      "が\tsample_doc10.txt 6\n",
      "が\tsample_doc3.txt 3\n",
      "が\tsample_doc4.txt 4\n",
      "が\tsample_doc5.txt 1\n",
      "が\tsample_doc6.txt 9\n",
      "が\tsample_doc7.txt 3\n",
      "が\tsample_doc8.txt 3\n",
      "が\tsample_doc9.txt 5\n",
      "くい\tsample_doc9.txt 1\n",
      "くずれ\tsample_doc9.txt 1\n",
      "くらい\tsample_doc7.txt 1\n",
      "くれる\tsample_doc8.txt 1\n",
      "くわし\tsample_doc4.txt 1\n",
      "けむり\tsample_doc5.txt 1\n",
      "こう\tsample_doc9.txt 1\n",
      "ここ\tsample_doc1.txt 1\n",
      "ここ\tsample_doc9.txt 1\n",
      "この\tsample_doc10.txt 2\n",
      "この\tsample_doc2.txt 1\n",
      "この\tsample_doc3.txt 1\n",
      "この\tsample_doc5.txt 1\n",
      "この\tsample_doc6.txt 1\n",
      "これ\tsample_doc5.txt 1\n",
      "これ\tsample_doc8.txt 1\n",
      "これから\tsample_doc10.txt 1\n",
      "こん\tsample_doc10.txt 1\n",
      "こんな\tsample_doc4.txt 1\n",
      "ご\tsample_doc4.txt 1\n",
      "さ\tsample_doc10.txt 1\n",
      "さ\tsample_doc4.txt 1\n",
      "さえ\tsample_doc7.txt 1\n",
      "さて\tsample_doc10.txt 1\n",
      "さらさら\tsample_doc9.txt 1\n",
      "さり\tsample_doc6.txt 1\n",
      "し\tsample_doc1.txt 2\n",
      "し\tsample_doc10.txt 2\n",
      "し\tsample_doc3.txt 1\n",
      "し\tsample_doc4.txt 2\n",
      "し\tsample_doc6.txt 3\n",
      "し\tsample_doc8.txt 2\n",
      "し\tsample_doc9.txt 2\n",
      "しかし\tsample_doc2.txt 1\n",
      "しかも\tsample_doc2.txt 1\n",
      "しき\tsample_doc10.txt 1\n",
      "しばらく\tsample_doc6.txt 2\n",
      "しばらく\tsample_doc8.txt 1\n",
      "しまっ\tsample_doc7.txt 1\n",
      "じめじめ\tsample_doc1.txt 1\n",
      "す\tsample_doc7.txt 1\n",
      "する\tsample_doc10.txt 1\n",
      "する\tsample_doc6.txt 1\n",
      "ず\tsample_doc4.txt 1\n",
      "そう\tsample_doc10.txt 1\n",
      "そう\tsample_doc2.txt 1\n",
      "そうして\tsample_doc5.txt 1\n",
      "そこ\tsample_doc9.txt 1\n",
      "その\tsample_doc10.txt 1\n",
      "その\tsample_doc2.txt 1\n",
      "その\tsample_doc5.txt 1\n",
      "その\tsample_doc7.txt 1\n",
      "そのうち\tsample_doc9.txt 1\n",
      "その後\tsample_doc4.txt 1\n",
      "それ\tsample_doc2.txt 1\n",
      "それ\tsample_doc6.txt 1\n",
      "そろ\tsample_doc9.txt 1\n",
      "そろりと\tsample_doc9.txt 1\n",
      "た\tsample_doc1.txt 3\n",
      "た\tsample_doc10.txt 4\n",
      "た\tsample_doc2.txt 3\n",
      "た\tsample_doc3.txt 5\n",
      "た\tsample_doc4.txt 2\n",
      "た\tsample_doc5.txt 2\n",
      "た\tsample_doc6.txt 3\n",
      "た\tsample_doc7.txt 3\n",
      "た\tsample_doc8.txt 3\n",
      "た\tsample_doc9.txt 4\n",
      "たく\tsample_doc9.txt 1\n",
      "たくさん\tsample_doc7.txt 1\n",
      "ただ\tsample_doc3.txt 1\n",
      "たばこ\tsample_doc5.txt 1\n",
      "たら\tsample_doc8.txt 2\n",
      "たら\tsample_doc9.txt 1\n",
      "だ\tsample_doc10.txt 2\n",
      "だ\tsample_doc2.txt 1\n",
      "だ\tsample_doc3.txt 1\n",
      "だ\tsample_doc4.txt 1\n",
      "だ\tsample_doc7.txt 1\n",
      "だ\tsample_doc9.txt 1\n",
      "だいぶ\tsample_doc4.txt 1\n",
      "だけ\tsample_doc1.txt 1\n",
      "だけ\tsample_doc6.txt 1\n",
      "ち\tsample_doc10.txt 1\n",
      "ついに\tsample_doc10.txt 1\n",
      "つか\tsample_doc1.txt 1\n",
      "つかまえ\tsample_doc2.txt 1\n",
      "つるつる\tsample_doc4.txt 1\n",
      "て\tsample_doc1.txt 3\n",
      "て\tsample_doc10.txt 5\n",
      "て\tsample_doc2.txt 1\n",
      "て\tsample_doc3.txt 4\n",
      "て\tsample_doc4.txt 2\n",
      "て\tsample_doc5.txt 1\n",
      "て\tsample_doc6.txt 4\n",
      "て\tsample_doc7.txt 5\n",
      "て\tsample_doc8.txt 5\n",
      "て\tsample_doc9.txt 7\n",
      "で\tsample_doc1.txt 4\n",
      "で\tsample_doc10.txt 3\n",
      "で\tsample_doc2.txt 4\n",
      "で\tsample_doc3.txt 3\n",
      "で\tsample_doc4.txt 1\n",
      "で\tsample_doc5.txt 1\n",
      "で\tsample_doc6.txt 2\n",
      "で\tsample_doc7.txt 1\n",
      "で\tsample_doc8.txt 1\n",
      "で\tsample_doc9.txt 2\n",
      "でも\tsample_doc1.txt 1\n",
      "でも\tsample_doc3.txt 1\n",
      "でも\tsample_doc7.txt 1\n",
      "と\tsample_doc10.txt 3\n",
      "と\tsample_doc2.txt 1\n",
      "と\tsample_doc3.txt 2\n",
      "と\tsample_doc6.txt 4\n",
      "と\tsample_doc7.txt 4\n",
      "と\tsample_doc8.txt 4\n",
      "と\tsample_doc9.txt 4\n",
      "とある\tsample_doc9.txt 1\n",
      "という\tsample_doc1.txt 1\n",
      "という\tsample_doc10.txt 1\n",
      "という\tsample_doc2.txt 4\n",
      "という\tsample_doc3.txt 1\n",
      "という\tsample_doc5.txt 1\n",
      "という\tsample_doc8.txt 1\n",
      "とうてい\tsample_doc6.txt 1\n",
      "として\tsample_doc6.txt 1\n",
      "とにかく\tsample_doc10.txt 1\n",
      "とんと\tsample_doc1.txt 1\n",
      "ど\tsample_doc6.txt 1\n",
      "どう\tsample_doc8.txt 1\n",
      "どうして\tsample_doc10.txt 1\n",
      "どうにか\tsample_doc9.txt 1\n",
      "どうも\tsample_doc5.txt 1\n",
      "どうも\tsample_doc9.txt 1\n",
      "どこ\tsample_doc1.txt 1\n",
      "な\tsample_doc10.txt 2\n",
      "な\tsample_doc2.txt 1\n",
      "な\tsample_doc3.txt 1\n",
      "な\tsample_doc6.txt 1\n",
      "ない\tsample_doc10.txt 2\n",
      "ない\tsample_doc4.txt 1\n",
      "ない\tsample_doc6.txt 3\n",
      "ない\tsample_doc7.txt 1\n",
      "ない\tsample_doc8.txt 2\n",
      "ない\tsample_doc9.txt 2\n",
      "なかっ\tsample_doc10.txt 1\n",
      "なかっ\tsample_doc2.txt 2\n",
      "なく\tsample_doc10.txt 1\n",
      "なっ\tsample_doc10.txt 2\n",
      "なら\tsample_doc10.txt 1\n",
      "なら\tsample_doc4.txt 1\n",
      "なり\tsample_doc10.txt 1\n",
      "なる\tsample_doc10.txt 1\n",
      "なる\tsample_doc6.txt 1\n",
      "なる\tsample_doc9.txt 1\n",
      "に\tsample_doc10.txt 5\n",
      "に\tsample_doc4.txt 1\n",
      "に\tsample_doc6.txt 2\n",
      "に\tsample_doc7.txt 3\n",
      "に\tsample_doc8.txt 4\n",
      "に\tsample_doc9.txt 5\n",
      "にて\tsample_doc2.txt 1\n",
      "ぬ\tsample_doc1.txt 1\n",
      "ぬ\tsample_doc7.txt 2\n",
      "の\tsample_doc10.txt 6\n",
      "の\tsample_doc2.txt 1\n",
      "の\tsample_doc3.txt 6\n",
      "の\tsample_doc4.txt 2\n",
      "の\tsample_doc5.txt 2\n",
      "の\tsample_doc6.txt 5\n",
      "の\tsample_doc7.txt 5\n",
      "の\tsample_doc8.txt 2\n",
      "の\tsample_doc9.txt 4\n",
      "のそのそ\tsample_doc7.txt 1\n",
      "のみ\tsample_doc4.txt 1\n",
      "は\tsample_doc1.txt 4\n",
      "は\tsample_doc10.txt 7\n",
      "は\tsample_doc2.txt 3\n",
      "は\tsample_doc4.txt 1\n",
      "は\tsample_doc5.txt 1\n",
      "は\tsample_doc6.txt 3\n",
      "は\tsample_doc7.txt 3\n",
      "は\tsample_doc8.txt 1\n",
      "は\tsample_doc9.txt 1\n",
      "はい出し\tsample_doc7.txt 1\n",
      "はじめ\tsample_doc3.txt 1\n",
      "はず\tsample_doc4.txt 1\n",
      "はっ\tsample_doc9.txt 1\n",
      "はてな\tsample_doc7.txt 1\n",
      "ばかり\tsample_doc3.txt 1\n",
      "ひだり\tsample_doc9.txt 1\n",
      "ひらに\tsample_doc3.txt 1\n",
      "ぴき\tsample_doc7.txt 1\n",
      "ふと\tsample_doc7.txt 1\n",
      "ふん\tsample_doc8.txt 1\n",
      "ぷうぷうと\tsample_doc5.txt 1\n",
      "へ\tsample_doc10.txt 3\n",
      "へ\tsample_doc7.txt 1\n",
      "へ\tsample_doc9.txt 2\n",
      "べき\tsample_doc4.txt 1\n",
      "べつ\tsample_doc8.txt 1\n",
      "ぼう\tsample_doc10.txt 1\n",
      "ぽく\tsample_doc5.txt 1\n",
      "また\tsample_doc8.txt 1\n",
      "まだ\tsample_doc1.txt 1\n",
      "まで\tsample_doc10.txt 1\n",
      "まで\tsample_doc6.txt 1\n",
      "まで\tsample_doc7.txt 1\n",
      "まで\tsample_doc9.txt 1\n",
      "まるで\tsample_doc4.txt 1\n",
      "み\tsample_doc3.txt 1\n",
      "むせ\tsample_doc5.txt 1\n",
      "むやみ\tsample_doc6.txt 1\n",
      "むやみ\tsample_doc7.txt 1\n",
      "も\tsample_doc2.txt 1\n",
      "も\tsample_doc4.txt 2\n",
      "も\tsample_doc6.txt 1\n",
      "も\tsample_doc7.txt 1\n",
      "も\tsample_doc8.txt 2\n",
      "も\tsample_doc9.txt 2\n",
      "もう\tsample_doc10.txt 1\n",
      "もぐり込ん\tsample_doc9.txt 1\n",
      "もし\tsample_doc10.txt 1\n",
      "もの\tsample_doc1.txt 1\n",
      "もの\tsample_doc10.txt 2\n",
      "もの\tsample_doc3.txt 2\n",
      "もの\tsample_doc5.txt 1\n",
      "もの\tsample_doc9.txt 1\n",
      "ものの\tsample_doc10.txt 1\n",
      "や\tsample_doc10.txt 1\n",
      "や\tsample_doc4.txt 1\n",
      "やっ\tsample_doc8.txt 1\n",
      "やら\tsample_doc6.txt 1\n",
      "ゆう\tsample_doc10.txt 1\n",
      "よ\tsample_doc10.txt 1\n",
      "よい\tsample_doc6.txt 1\n",
      "よい\tsample_doc9.txt 1\n",
      "よう\tsample_doc7.txt 1\n",
      "ようやく\tsample_doc5.txt 1\n",
      "ようやく\tsample_doc8.txt 1\n",
      "ようやく\tsample_doc9.txt 1\n",
      "よかろ\tsample_doc8.txt 1\n",
      "よく\tsample_doc10.txt 1\n",
      "られ\tsample_doc3.txt 2\n",
      "られ\tsample_doc7.txt 2\n",
      "り\tsample_doc9.txt 1\n",
      "れ\tsample_doc4.txt 1\n",
      "ろ\tsample_doc10.txt 1\n",
      "わに\tsample_doc4.txt 1\n",
      "わら\tsample_doc7.txt 1\n",
      "を\tsample_doc1.txt 1\n",
      "を\tsample_doc10.txt 1\n",
      "を\tsample_doc2.txt 1\n",
      "を\tsample_doc3.txt 1\n",
      "を\tsample_doc5.txt 1\n",
      "を\tsample_doc7.txt 2\n",
      "を\tsample_doc8.txt 1\n",
      "を\tsample_doc9.txt 4\n",
      "をもって\tsample_doc4.txt 1\n",
      "ん\tsample_doc10.txt 1\n",
      "スー\tsample_doc3.txt 1\n",
      "ニャー\tsample_doc8.txt 2\n",
      "ニャーニャー\tsample_doc1.txt 1\n",
      "フワフワ\tsample_doc3.txt 1\n",
      "一\tsample_doc4.txt 2\n",
      "一刻\tsample_doc10.txt 1\n",
      "一樹\tsample_doc10.txt 1\n",
      "一番\tsample_doc2.txt 1\n",
      "一疋\tsample_doc7.txt 1\n",
      "三\tsample_doc10.txt 1\n",
      "上\tsample_doc3.txt 1\n",
      "上\tsample_doc7.txt 1\n",
      "上\tsample_doc9.txt 1\n",
      "上今\tsample_doc7.txt 1\n",
      "不思議\tsample_doc10.txt 1\n",
      "中\tsample_doc2.txt 1\n",
      "中\tsample_doc5.txt 1\n",
      "中\tsample_doc7.txt 1\n",
      "事\tsample_doc4.txt 1\n",
      "事\tsample_doc5.txt 1\n",
      "事\tsample_doc6.txt 1\n",
      "事\tsample_doc9.txt 1\n",
      "云\tsample_doc10.txt 1\n",
      "人間\tsample_doc1.txt 1\n",
      "人間\tsample_doc2.txt 1\n",
      "人間\tsample_doc3.txt 1\n",
      "人間\tsample_doc5.txt 1\n",
      "人間\tsample_doc9.txt 1\n",
      "今\tsample_doc3.txt 1\n",
      "今日\tsample_doc10.txt 1\n",
      "仕方\tsample_doc10.txt 1\n",
      "仕方\tsample_doc9.txt 1\n",
      "付い\tsample_doc7.txt 1\n",
      "何\tsample_doc1.txt 1\n",
      "何\tsample_doc2.txt 1\n",
      "何\tsample_doc6.txt 1\n",
      "何\tsample_doc7.txt 1\n",
      "何\tsample_doc9.txt 1\n",
      "何だか\tsample_doc3.txt 1\n",
      "何となく\tsample_doc9.txt 1\n",
      "兄弟\tsample_doc7.txt 1\n",
      "先\tsample_doc10.txt 1\n",
      "内\tsample_doc9.txt 1\n",
      "出\tsample_doc6.txt 1\n",
      "出\tsample_doc8.txt 1\n",
      "出\tsample_doc9.txt 2\n",
      "出会\tsample_doc4.txt 1\n",
      "出来\tsample_doc10.txt 1\n",
      "分ら\tsample_doc10.txt 1\n",
      "分ら\tsample_doc6.txt 2\n",
      "分別\tsample_doc8.txt 1\n",
      "別に\tsample_doc8.txt 1\n",
      "別段\tsample_doc2.txt 1\n",
      "到底\tsample_doc6.txt 1\n",
      "前\tsample_doc8.txt 1\n",
      "助から\tsample_doc6.txt 1\n",
      "動く\tsample_doc6.txt 2\n",
      "名前\tsample_doc1.txt 1\n",
      "向う\tsample_doc8.txt 1\n",
      "吹く\tsample_doc5.txt 1\n",
      "吾輩\tsample_doc1.txt 2\n",
      "吾輩\tsample_doc10.txt 2\n",
      "吾輩\tsample_doc7.txt 1\n",
      "吾輩\tsample_doc8.txt 1\n",
      "咽\tsample_doc5.txt 1\n",
      "善い\tsample_doc10.txt 1\n",
      "坐っ\tsample_doc6.txt 1\n",
      "坐っ\tsample_doc8.txt 1\n",
      "垣根\tsample_doc10.txt 1\n",
      "声\tsample_doc9.txt 1\n",
      "大きな\tsample_doc8.txt 1\n",
      "妙\tsample_doc3.txt 1\n",
      "始\tsample_doc3.txt 1\n",
      "始め\tsample_doc1.txt 1\n",
      "始め\tsample_doc6.txt 1\n",
      "始め\tsample_doc9.txt 1\n",
      "始末\tsample_doc10.txt 1\n",
      "姿\tsample_doc7.txt 1\n",
      "実に\tsample_doc5.txt 1\n",
      "容子\tsample_doc7.txt 1\n",
      "寒\tsample_doc10.txt 1\n",
      "寒し\tsample_doc10.txt 1\n",
      "少し\tsample_doc3.txt 1\n",
      "崩\tsample_doc9.txt 1\n",
      "左\tsample_doc9.txt 1\n",
      "度\tsample_doc4.txt 1\n",
      "廻り\tsample_doc9.txt 1\n",
      "廻る\tsample_doc6.txt 1\n",
      "弱っ\tsample_doc5.txt 1\n",
      "当時\tsample_doc2.txt 1\n",
      "彼\tsample_doc3.txt 1\n",
      "心持\tsample_doc6.txt 1\n",
      "忍び込ん\tsample_doc10.txt 1\n",
      "思い\tsample_doc8.txt 1\n",
      "思っ\tsample_doc3.txt 1\n",
      "思っ\tsample_doc6.txt 1\n",
      "思っ\tsample_doc9.txt 1\n",
      "思わ\tsample_doc2.txt 1\n",
      "急\tsample_doc7.txt 1\n",
      "恐し\tsample_doc2.txt 1\n",
      "悪く\tsample_doc6.txt 1\n",
      "感じ\tsample_doc3.txt 2\n",
      "我々\tsample_doc2.txt 1\n",
      "我慢\tsample_doc9.txt 1\n",
      "所\tsample_doc1.txt 1\n",
      "所\tsample_doc7.txt 1\n",
      "所\tsample_doc9.txt 2\n",
      "持ち上げ\tsample_doc3.txt 1\n",
      "捕\tsample_doc2.txt 1\n",
      "掌\tsample_doc3.txt 2\n",
      "掌\tsample_doc6.txt 1\n",
      "方\tsample_doc10.txt 2\n",
      "日\tsample_doc9.txt 1\n",
      "明い\tsample_doc7.txt 1\n",
      "明るい\tsample_doc7.txt 1\n",
      "明るく\tsample_doc10.txt 1\n",
      "時\tsample_doc10.txt 1\n",
      "時\tsample_doc3.txt 2\n",
      "時々\tsample_doc2.txt 1\n",
      "時々\tsample_doc5.txt 1\n",
      "暖か\tsample_doc10.txt 1\n",
      "暗\tsample_doc6.txt 1\n",
      "暗\tsample_doc7.txt 1\n",
      "暗く\tsample_doc10.txt 1\n",
      "暮れ\tsample_doc9.txt 1\n",
      "書生\tsample_doc2.txt 2\n",
      "書生\tsample_doc3.txt 1\n",
      "書生\tsample_doc6.txt 2\n",
      "書生\tsample_doc7.txt 1\n",
      "書生\tsample_doc8.txt 1\n",
      "来\tsample_doc8.txt 2\n",
      "来\tsample_doc9.txt 1\n",
      "来る\tsample_doc10.txt 1\n",
      "棄て\tsample_doc7.txt 1\n",
      "残っ\tsample_doc3.txt 1\n",
      "母親\tsample_doc7.txt 1\n",
      "毛\tsample_doc10.txt 1\n",
      "毛\tsample_doc4.txt 1\n",
      "気\tsample_doc7.txt 1\n",
      "池\tsample_doc8.txt 2\n",
      "池\tsample_doc9.txt 2\n",
      "決心\tsample_doc9.txt 1\n",
      "泣い\tsample_doc1.txt 1\n",
      "泣い\tsample_doc8.txt 1\n",
      "泣き\tsample_doc9.txt 1\n",
      "減っ\tsample_doc9.txt 1\n",
      "減る\tsample_doc10.txt 1\n",
      "渡っ\tsample_doc9.txt 1\n",
      "火\tsample_doc6.txt 1\n",
      "無\tsample_doc6.txt 1\n",
      "無\tsample_doc7.txt 1\n",
      "無い\tsample_doc1.txt 1\n",
      "無理やり\tsample_doc9.txt 1\n",
      "煙\tsample_doc5.txt 1\n",
      "煙草\tsample_doc5.txt 1\n",
      "煮\tsample_doc2.txt 1\n",
      "片\tsample_doc4.txt 1\n",
      "猫\tsample_doc1.txt 1\n",
      "猫\tsample_doc4.txt 1\n",
      "猶予\tsample_doc10.txt 1\n",
      "獰悪\tsample_doc2.txt 1\n",
      "生れ\tsample_doc1.txt 1\n",
      "痛い\tsample_doc7.txt 1\n",
      "真中\tsample_doc4.txt 1\n",
      "眼\tsample_doc6.txt 2\n",
      "眼\tsample_doc7.txt 1\n",
      "知っ\tsample_doc5.txt 1\n",
      "知れ\tsample_doc10.txt 1\n",
      "破れ\tsample_doc10.txt 1\n",
      "種族\tsample_doc2.txt 1\n",
      "穴\tsample_doc10.txt 1\n",
      "穴\tsample_doc5.txt 1\n",
      "穴\tsample_doc9.txt 1\n",
      "突起\tsample_doc4.txt 1\n",
      "竹垣\tsample_doc10.txt 1\n",
      "竹垣\tsample_doc9.txt 1\n",
      "第\tsample_doc4.txt 1\n",
      "笹原\tsample_doc7.txt 1\n",
      "笹原\tsample_doc8.txt 1\n",
      "縁\tsample_doc10.txt 1\n",
      "考\tsample_doc2.txt 1\n",
      "考え\tsample_doc8.txt 1\n",
      "考え付い\tsample_doc8.txt 1\n",
      "考え出そ\tsample_doc6.txt 1\n",
      "聞く\tsample_doc2.txt 1\n",
      "肝心\tsample_doc7.txt 1\n",
      "胸\tsample_doc6.txt 1\n",
      "腹\tsample_doc10.txt 1\n",
      "腹\tsample_doc9.txt 1\n",
      "自分\tsample_doc6.txt 1\n",
      "臭い\tsample_doc9.txt 1\n",
      "至る\tsample_doc10.txt 1\n",
      "苦しい\tsample_doc9.txt 1\n",
      "落ちつい\tsample_doc3.txt 1\n",
      "蔭\tsample_doc10.txt 1\n",
      "薄暗い\tsample_doc1.txt 1\n",
      "薬缶\tsample_doc4.txt 1\n",
      "藁\tsample_doc7.txt 1\n",
      "行く\tsample_doc10.txt 1\n",
      "行く\tsample_doc9.txt 1\n",
      "装飾\tsample_doc4.txt 1\n",
      "裏\tsample_doc6.txt 1\n",
      "見\tsample_doc1.txt 1\n",
      "見\tsample_doc3.txt 2\n",
      "見\tsample_doc8.txt 2\n",
      "見え\tsample_doc7.txt 1\n",
      "見る\tsample_doc7.txt 2\n",
      "見当\tsample_doc1.txt 1\n",
      "記憶\tsample_doc1.txt 1\n",
      "記憶\tsample_doc6.txt 1\n",
      "訪問\tsample_doc10.txt 1\n",
      "試み\tsample_doc8.txt 1\n",
      "話\tsample_doc2.txt 1\n",
      "誰\tsample_doc8.txt 1\n",
      "路傍\tsample_doc10.txt 1\n",
      "載せ\tsample_doc3.txt 1\n",
      "輪\tsample_doc4.txt 1\n",
      "迎\tsample_doc8.txt 1\n",
      "這\tsample_doc7.txt 1\n",
      "這\tsample_doc9.txt 1\n",
      "這い出す\tsample_doc8.txt 1\n",
      "這入\tsample_doc9.txt 1\n",
      "通路\tsample_doc10.txt 1\n",
      "速力\tsample_doc6.txt 1\n",
      "逢\tsample_doc4.txt 1\n",
      "運転\tsample_doc6.txt 1\n",
      "違っ\tsample_doc7.txt 1\n",
      "邸\tsample_doc10.txt 1\n",
      "邸\tsample_doc9.txt 1\n",
      "降っ\tsample_doc10.txt 1\n",
      "隠し\tsample_doc7.txt 1\n",
      "隣家\tsample_doc10.txt 1\n",
      "雨\tsample_doc10.txt 1\n",
      "非常\tsample_doc6.txt 1\n",
      "非常\tsample_doc7.txt 1\n",
      "非常\tsample_doc9.txt 2\n",
      "音\tsample_doc6.txt 1\n",
      "頃\tsample_doc5.txt 1\n",
      "顔\tsample_doc3.txt 1\n",
      "顔\tsample_doc4.txt 2\n",
      "風\tsample_doc9.txt 1\n",
      "食う\tsample_doc2.txt 1\n",
      "食物\tsample_doc9.txt 1\n",
      "飲む\tsample_doc5.txt 1\n",
      "餓死\tsample_doc10.txt 1\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリを読み込む\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "\n",
    "# データフォルダの設定\n",
    "DATA = \"../data\"\n",
    "\n",
    "# 辞書オブジェクトの初期化\n",
    "dict = {}\n",
    "\n",
    "# DATAフォルダに含まれるファイルを一つずつ処理\n",
    "for filename in os.listdir(DATA):\n",
    "    f = open(DATA + '/' + filename, 'r')\n",
    "    \n",
    "    # ファイルを1行ずつ処理\n",
    "    for line in f:\n",
    "        tokens = t.tokenize(line)\n",
    "        # 分かち書きした単語を一つずつ処理\n",
    "        for token in tokens:\n",
    "            # もしdictに単語をキーとするレコードが存在していれば\n",
    "            if token.surface in dict:\n",
    "                # もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "                if filename in dict[token.surface]:\n",
    "                    dict[token.surface][filename] += 1\n",
    "\n",
    "                # さもなくば\n",
    "                else:\n",
    "                    dict[token.surface][filename] = 1\n",
    "\n",
    "            # さもなくば\n",
    "            else:\n",
    "                dict[token.surface] = {}\n",
    "                dict[token.surface][filename] = 1\n",
    "            \n",
    "\n",
    "#全体の出力（ソート）\n",
    "for key in sorted(dict):\n",
    "    #print(key + '\\t' + str(dict[key]))\n",
    "    for all in sorted(dict[key]):\n",
    "        print(key + '\\t' + all + \" \" + str(dict[key][all]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> `、\tsample_doc10.txt\t5`  \n",
    "> `、\tsample_doc6.txt\t2`  \n",
    "> `、\tsample_doc7.txt\t1`  \n",
    "> `、\tsample_doc8.txt\t1`  \n",
    "> `、\tsample_doc9.txt\t3`  \n",
    "> ...  \n",
    "\n",
    "という出力が出れば成功です。少しわかりづらいですが、句読点「、」が`sample_doc10.txt`に5回、`sample_doc6.txt`に2回出現していることが読み取れます。\n",
    "\n",
    "#### 上手くいかない場合1\n",
    "`NameError`というエラーが出た人は、演習用フォルダ構成を確認しましょう。\n",
    "\n",
    "#### 上手くいかない場合2\n",
    "レコードは出力されるけれど、結果がおかしいという人は以下を確認してください。\n",
    "\n",
    "> 既にキー2（と値）のレコードをもつキー1を初期化していないか\n",
    "\n",
    "ネストした辞書オブジェクトを初期化するのは、キー1を初めて追加するときのみです。それ以降のデータ追加に初期化してしまうと、それまで追加してきたキー2と値のレコードが消えてしまいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 索引語をあいうえお順に並べる（ソートの説明詳細版）\n",
    "\n",
    "さて、先のプログラムで単語がどのファイルに何回出現するのかを数えることができました。\n",
    "\n",
    "> だいぶ目指している本の後ろにある索引に近づいてきたので、これからは**単語**ではなく、**索引語**と呼んでいきましょう。\n",
    "\n",
    "本の後ろにある索引は通常あいうえお順に並んでいます。そうしないと、長いリストから索引語を見つけることができません。しかし、以前にも説明しましたが、**辞書オブジェクトは順序を保持しない**ので、キーが入力順に並んで保存されている保証もありません。\n",
    "\n",
    "というわけで、次に、あいうえお順にならんだ索引語リストを出力するプログラムを書いていきましょう。\n",
    "\n",
    "あいうえお順にならべる処理は、辞書オブジェクトの出力処理の部分で行います。↓の部分です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 全体の出力\n",
    "for key1 in dict:\n",
    "    for key2 in dict[key1]:\n",
    "        print(key1 + '\\t' + key2 + '\\t' + str(dict[key1][key2]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで何度も使ってきているように、\n",
    "\n",
    "> `for`ループは、`dict`オブジェクトからキーを一つずつ受け取り、それ以降の処理をします\n",
    "\n",
    "なので、`dict`オブジェクトが値を渡す時に、キーの順序がアルファベット順になっていれば、望む出力になりそうです。\n",
    "\n",
    "`dict`オブジェクトのキーをアルファベット順にならべて取り出すには`sorted`メソッドを使います。\n",
    "\n",
    "> ちなみに、値や文字列を並び替えることを**ソート（sort）**といいます。\n",
    "\n",
    "例を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['いす', 'かんがるー', 'ひと']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 辞書オブジェクトの初期化とデータ追加\n",
    "# 追加された順序はバラバラ\n",
    "dict = {\n",
    "    'かんがるー': 3,\n",
    "    'いす': 4,\n",
    "    'ひと': 2\n",
    "}\n",
    "# ソートされたキーの出力\n",
    "sorted(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムを実行すると\n",
    "\n",
    "> ['いす', 'かんがるー', 'ひと']\n",
    "\n",
    "というように`dict`オブジェクトのキーがあいうえお順に並んで出力されます。値は出力されません。これを、`for`ループで上手く使えば、目指すプログラムになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 不要語の削除\n",
    "\n",
    "さて、索引付けもいよいよ**最終段階**です。\n",
    "\n",
    "前回までの処理でほぼ索引語一覧として望むものを手に入れることができました。しかし、出力結果を見てみると、**ノイズがたくさんある**のもまた事実です。ノイズとは、**検索に使われる可能性の低い不要語**のことです。\n",
    "\n",
    "例えば、\n",
    "\n",
    "> ``。\tsample_doc1.txt\t5``  \n",
    "> ``か\tsample_doc1.txt\t1``  \n",
    "> ``という\tsample_doc1.txt\t1``\n",
    "\n",
    "などは、検索に有用である可能性は低いと考えられます。\n",
    "\n",
    "ですので、これら不要語を索引付け処理から除く方法を考えていきましょう。\n",
    "\n",
    "> ところで、現在の検索エンジンでは、必ずしも不要語の削除をしていません。不要語も有用なデータとして活用する、より高度な順位付け方法が開発されてきたからです。しかし、本演習では、プログラミングの練習も兼ねていますので、不要語の削除処理を取り入れることにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### どの段階で削除するのがよいのか？\n",
    "\n",
    "不要語削除の処理は、これまで作成してきたプログラムのどの段階で実行するのがよいのでしょうか？\n",
    "\n",
    "これも色々な考え方があると思いますが、本演習では**辞書オブジェクトに取り込む直前に削除する**方針を採用したいと思います。\n",
    "\n",
    "↓の部分です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    for line in f:\n",
    "        tokens = t.tokenize(line)\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            ## ここに不要語削除の処理を入れる\n",
    "\n",
    "            if token.surface in dict:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 何を不要語とするのか？\n",
    "\n",
    "次に考えるべきことは、何を不要語と定義するのか、という問いです。\n",
    "\n",
    "ぱっと思いつくのは、**句読点**でしょうか。\n",
    "\n",
    "あとは、**1文字の語**も役に立ちそうにありません。とはいえ、「猫」は一文字語ですが、是非検索語に使いたい語です。ということは、不要なのは**一文字のひらがな**でしょうか。\n",
    "\n",
    "後は何が考えられますか？\n",
    "\n",
    "本演習では、後から不要語を追加できるような、柔軟な解決方法を学んでいきましょう。\n",
    "\n",
    "まずは手始めとして、\n",
    "\n",
    "- 句読点\n",
    "- 一文字のひらがな\n",
    "\n",
    "を不要語と定義します。\n",
    "\n",
    "> ちなみに、不要語は英語でストップワード（stopword）と呼ばれ、色々な団体がストップワードのリストを公開しています。興味のある方は、探してみましょう。  \n",
    "> また、日本語では、先に実行した分かち書き処理で入手可能な**品詞情報**をつかって、不要語の削除を行うことも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 正規表現を用いた不要語の削除方法\n",
    "\n",
    "不要語の（暫定的な）定義がかたまったところで、不要語削除の処理をする方法を考えましょう。\n",
    "\n",
    "ここでは、**正規表現**という手法に活躍してもらいます。\n",
    "\n",
    "> 正規表現は、**パターンマッチ**と呼ばれることもあります。\n",
    "\n",
    "まずは、正規表現の例をいくつか見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 2), match='京都'>\n",
      "None\n",
      "None\n",
      "<_sre.SRE_Match object; span=(1, 3), match='京都'>\n"
     ]
    }
   ],
   "source": [
    "# reパッケージの読み込み\n",
    "import re\n",
    "\n",
    "# マッチしたいパターンの定義\n",
    "pattern = re.compile(r\"京都\")\n",
    "\n",
    "# 対象となる検索語\n",
    "token1 = \"京都\"\n",
    "token2 = \"大阪\"\n",
    "token3 = \"東京都\"\n",
    "\n",
    "# マッチングの実行と結果出力\n",
    "print(pattern.match(token1))\n",
    "print(pattern.match(token2))\n",
    "print(pattern.match(token3))\n",
    "print(pattern.search(token3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムは、\n",
    "\n",
    "> 2: 正規表現をあつかうreパッケージの読み込み  \n",
    "> 5: マッチさせたいパターンの定義。ここでは**京都**という文字列にマッチするパターンを定義しています。**パターンの前に`r`がある**点に注意してください。また、`compile`メソッドは、定義したパターンを高速にマッチできるようにコンパイルして、変数`pattern`に格納しています。  \n",
    "> 8-10: マッチ対象となる検索語の定義  \n",
    "> 13-15: `match`メソッドをつかった、正規表現と検索語のマッチング  \n",
    "> 16: `search`メソッドをつかった、正規表現と検索語のマッチング\n",
    "\n",
    "の処理を行っています。\n",
    "\n",
    "それでは、プログラムを実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムを実行すると\n",
    "\n",
    "> `<_sre.SRE_Match object; span=(0, 2), match='京都'>`  \n",
    "> `None`  \n",
    "> `None`  \n",
    "> `<_sre.SRE_Match object; span=(1, 3), match='京都'>`\n",
    "\n",
    "という結果が出力されます。\n",
    "\n",
    "一つ目は、**京都**と**京都**なのでマッチします。\n",
    "\n",
    "一方、二つ目は、**京都**と**大阪**なのでマッチしません。\n",
    "\n",
    "三つ目は、**京都**と東**京都**ですがマッチしません。`match`メソッドは、対象文字列の**先頭からマッチ**する必要があります。\n",
    "\n",
    "一方。四つ目の`search`メソッドは、対象文字列の**いかなる場所にもマッチ**するので、マッチしたとの結果が出力されました。\n",
    "\n",
    "> `match`メソッドと`search`メソッドの振る舞いの違いに注意しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 正規表現の黒魔術に触れる\n",
    "\n",
    "正規表現には、黒魔術のような側面があります。何故なら、パターン表現に様々な記号を使うからです。以下はその例です。\n",
    "\n",
    "| パターン | マッチする条件 |\n",
    "| :--:    | :--  |\n",
    "| `r\".\"`     | あらゆる1文字（空白文字、改行文字も含む） |\n",
    "| `r\"\\d\"`     | 0-9までの1文字 |\n",
    "| `r\"\\d+\"`     | 0-9までの文字の連続（他の文字種は含まない） |\n",
    "| `r\"\\w\"`     | 数字、アルファベット（大文字・小文字を含む）、ひらがな、漢字、アンダースコア（`_`）の1文字 |\n",
    "| `r\"\\W\"`     | 上記以外の1文字（おもにアンダースコア以外の記号文字など） |\n",
    "| `r\"^\"`     | 対象文字列の先頭 |\n",
    "| `r\"$\"`     | 対象文字列の末尾（末尾に改行文字があれば改行文字にマッチ） |\n",
    "| `r\"[あ-ん]\"`  | （あ→んまでの）ひらがな1文字 |\n",
    "| `r\"[あ-んア-ン]\"` | （あ→んまでの）ひらがな、（ア→ンまでの）カタカナ1文字 |\n",
    "| `r\"[　-ー]\"`  | 全角スペース、ひらがな、カタカナ、長音記号、句読点1文字。 |\n",
    "\n",
    "> 本演習では、それほど複雑な表現を必要としませんが、正規表現は**非常に奥が深い**技術で、使いこなせると非常に便利です。興味のある方は、色々な表現を調べてみましょう。\n",
    "\n",
    "さて、それでは、先にあげた3つの不要語の例をつかって、これらにマッチする正規表現を考えていきましょう。\n",
    "\n",
    "- **。**：句読点1文字\n",
    "- **か**：ひらがな1文字\n",
    "- **という**：ひらがな3文字\n",
    "\n",
    "1番目と2番目の例は、両方とも1文字です。しかし、`r\".\"`を使って一文字で構成される索引語をすべて除いてしまうと、**猫**のような索引語も除かれてしまいます。\n",
    "\n",
    "一方で、`r\"[　-ー]\"`を使うと、ひらがな、カタカナ、句読点などが一文字で構成する索引語にマッチできそうです。この表現は、**（見えない）全角スペースが入っているので、コピペしましょう。**\n",
    "\n",
    "それでは、プログラムを書いてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 1), match='。'>\n",
      "<_sre.SRE_Match object; span=(0, 1), match='か'>\n",
      "None\n",
      "<_sre.SRE_Match object; span=(0, 1), match='か'>\n"
     ]
    }
   ],
   "source": [
    "# reパッケージの読み込み\n",
    "import re\n",
    "\n",
    "# マッチしたいパターンの定義\n",
    "pattern = re.compile(r\"[　-ー]\")\n",
    "\n",
    "# 対象となる検索語\n",
    "token1 = \"。\" # 不要語\n",
    "token2 = \"か\" # 不要語\n",
    "token3 = \"猫\" # 不要語ではない\n",
    "token4 = \"かさ\" # 不要語ではない\n",
    "\n",
    "# マッチングの実行と結果出力\n",
    "print(pattern.match(token1))\n",
    "print(pattern.match(token2))\n",
    "print(pattern.match(token3))\n",
    "print(pattern.match(token4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムを実行すると\n",
    "\n",
    "> `<_sre.SRE_Match object; span=(0, 1), match='。'>`   \n",
    "> `<_sre.SRE_Match object; span=(0, 1), match='か'>`  \n",
    "> `None`  \n",
    "> `<_sre.SRE_Match object; span=(0, 1), match='か'>`\n",
    "\n",
    "という結果が出ます。\n",
    "\n",
    "こちらの希望どおり、一文字で構成される句読点とひらがなにマッチしますが、漢字一文字にはマッチしませんでした。しかし、**かさ**にもマッチしてしまいました。\n",
    "\n",
    "どうしてでしょうか。\n",
    "\n",
    "理由は、**かさ**の一文字目の「か」が、ひらがな一文字で構成されるという条件と一致しているからです。\n",
    "\n",
    "我々が不要語として除きたいのは、**「か」ではじまり「か」でおわる一文字**の文字列です。正規表現の表からどの黒魔術を召喚すれば、「かさ」にマッチしないパターンを作成できるか考えてみましょう。そして、変更したパターン定義を使ったプログラムを作成し、実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 1), match='。'>\n",
      "<_sre.SRE_Match object; span=(0, 1), match='か'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# reパッケージの読み込み\n",
    "import re\n",
    "\n",
    "# 新しいパターンの定義 ↓\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "\n",
    "# 対象となる検索語\n",
    "token1 = \"。\" # 不要語\n",
    "token2 = \"か\" # 不要語\n",
    "token3 = \"猫\" # 不要語ではない\n",
    "token4 = \"かさ\" # 不要語ではない\n",
    "\n",
    "# マッチングの実行と結果出力\n",
    "print(pattern.match(token1))\n",
    "print(pattern.match(token2))\n",
    "print(pattern.match(token3))\n",
    "print(pattern.match(token4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> `<_sre.SRE_Match object; span=(0, 1), match='。'>`  \n",
    "> `<_sre.SRE_Match object; span=(0, 1), match='か'>`  \n",
    "> `None`  \n",
    "> `None`\n",
    "\n",
    "という出力が得られたら、成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 辞書オブジェクトを用いた不要語の削除方法\n",
    "\n",
    "正規表現をつかって\n",
    "\n",
    "> ``。\tsample_doc1.txt\t5``  \n",
    "> ``か\tsample_doc1.txt\t1``  \n",
    "> ``という\tsample_doc1.txt\t1``\n",
    "\n",
    "の最初の2つを捕まえることができましたが、3つ目はどうすればよいのでしょうか。\n",
    "\n",
    "このような不規則な長めの文字列は一致させなければならない条件が多くあるので、正規表現を上手く機能させることは難しいです（やってやれないことはないですが・・・）。\n",
    "\n",
    "そこで、これまで使ってきた**辞書オブジェクト**にまた活躍してもらいましょう。\n",
    "\n",
    "手順は以下の通りです。\n",
    "\n",
    "- 辞書オブジェクトを使って不要語リストの辞書を事前に作成する\n",
    "- 索引語が不要語リストに含まれているか否かを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要語辞書の初期化\n",
    "stopwords = {}\n",
    "\n",
    "# 不要語の追加\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムは\n",
    "\n",
    "> 2: 不要語用辞書オブジェクト`stopwords`を初期化  \n",
    "> 5-6: 不要語をキーとして`stopwords`オブジェクトにレコード追加。値は任意でよいが、ここでは`1`を使用。\n",
    "\n",
    "という内容です。\n",
    "\n",
    "> このような辞書を作成しておくことで、後から不要語の追加や削除が柔軟に行えます。\n",
    "\n",
    "ある文字列が不要語辞書に含まれているか否かの確認は、これまで作成してきたプログラムの中で類似の処理をしているので、そこから考えてみましょう。\n",
    "\n",
    "#### `for`ループ内で処理をスキップする方法\n",
    "索引付けのプログラムでは、分かち書きされた単語（索引語）を`for`ループを使って順番に処理していました。\n",
    "\n",
    "ある索引語は不要語の正規表現や不要語辞書に存在した場合は、その時点で`for`ループの処理を中断して、次の索引語にスキップすると、不要語が索引語辞書オブジェクトに含まれないで済みます。\n",
    "\n",
    "ループ内のスキップは以下のように`continue`メソッドを行います。\n",
    "\n",
    "> `else:`文をあえて書かないのがポイントです。こうすることで、不要語を除く処理（例：`if`ブロック）を複数重ねて書くことができます。索引語に含めるか否かのチェック機能を複数設定するイメージです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '条件文' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e9441bbe87be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 不要語辞書に存在した場合\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m条件文\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# 条件に一致したら残りの処理をスキップ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '条件文' is not defined"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    # 不要語辞書に存在した場合\n",
    "    if [条件文]:\n",
    "        # 条件に一致したら残りの処理をスキップ\n",
    "        continue\n",
    "    \n",
    "    # スキップしなかった場合の処理\n",
    "    print(token + \" は不要語ではありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "それでは、不要語辞書を定義して、検索語一覧から不要語辞書に含まれているかを確認し、含まれていない場合のみ出力するプログラムを書いてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "京都は不用語ではありません\n",
      "大阪は不用語ではありません\n"
     ]
    }
   ],
   "source": [
    "# 不要語辞書の初期化\n",
    "stopwords = {}\n",
    "\n",
    "# 不要語の追加\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "\n",
    "# 対象となる検索語の配列\n",
    "tokens = ['京都', '大阪', 'という', 'にて']\n",
    "\n",
    "# 不要語の確認とスキップ\n",
    "for token in tokens:\n",
    "    # 不要語辞書に存在した場合\n",
    "    if token in stopwords:\n",
    "        # 条件に一致したら残りの処理をスキップ\n",
    "        continue\n",
    "\n",
    "    # スキップしなかった場合の処理\n",
    "    print(token + \"は不用語ではありません\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> 京都 は不要語ではありません  \n",
    "> 大阪 は不要語ではありません\n",
    "\n",
    "という出力が得られたら、成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 不要語削除処理の統合\n",
    "\n",
    "それでは、正規表現と辞書オブジェクトを使った不要語処理機能が実装できたので、索引付け本体のプログラムに組み込みましょう。\n",
    "\n",
    "組み込む場所は、先の説明を参照してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あっ\tsample_doc2.txt 1\n",
      "あっ\tsample_doc3.txt 1\n",
      "あっ\tsample_doc4.txt 1\n",
      "あと\tsample_doc2.txt 1\n",
      "あと\tsample_doc6.txt 1\n",
      "あまりに\tsample_doc4.txt 1\n",
      "ある\tsample_doc1.txt 1\n",
      "ある\tsample_doc10.txt 1\n",
      "ある\tsample_doc2.txt 1\n",
      "ある\tsample_doc3.txt 1\n",
      "ある\tsample_doc5.txt 1\n",
      "ある\tsample_doc7.txt 1\n",
      "ある\tsample_doc8.txt 1\n",
      "ある\tsample_doc9.txt 2\n",
      "あるい\tsample_doc10.txt 1\n",
      "あろ\tsample_doc3.txt 1\n",
      "いか\tsample_doc10.txt 1\n",
      "いくら\tsample_doc6.txt 1\n",
      "いた事\tsample_doc1.txt 1\n",
      "いっ\tsample_doc10.txt 1\n",
      "いっ\tsample_doc9.txt 1\n",
      "いとも\tsample_doc2.txt 1\n",
      "いる\tsample_doc1.txt 1\n",
      "いる\tsample_doc10.txt 1\n",
      "いる\tsample_doc3.txt 1\n",
      "いる\tsample_doc4.txt 1\n",
      "いる\tsample_doc6.txt 2\n",
      "いわゆる\tsample_doc3.txt 1\n",
      "うち\tsample_doc10.txt 1\n",
      "うち\tsample_doc6.txt 1\n",
      "おかしい\tsample_doc7.txt 1\n",
      "おっ\tsample_doc6.txt 1\n",
      "おっ\tsample_doc7.txt 1\n",
      "かかる\tsample_doc9.txt 1\n",
      "かた\tsample_doc4.txt 1\n",
      "かも\tsample_doc10.txt 1\n",
      "から\tsample_doc10.txt 1\n",
      "から\tsample_doc2.txt 1\n",
      "から\tsample_doc5.txt 1\n",
      "から\tsample_doc6.txt 1\n",
      "から\tsample_doc7.txt 1\n",
      "から\tsample_doc9.txt 2\n",
      "かん\tsample_doc4.txt 1\n",
      "くい\tsample_doc9.txt 1\n",
      "くずれ\tsample_doc9.txt 1\n",
      "くらい\tsample_doc7.txt 1\n",
      "くれる\tsample_doc8.txt 1\n",
      "くわし\tsample_doc4.txt 1\n",
      "けむり\tsample_doc5.txt 1\n",
      "こう\tsample_doc9.txt 1\n",
      "ここ\tsample_doc1.txt 1\n",
      "ここ\tsample_doc9.txt 1\n",
      "この\tsample_doc10.txt 2\n",
      "この\tsample_doc2.txt 1\n",
      "この\tsample_doc3.txt 1\n",
      "この\tsample_doc5.txt 1\n",
      "この\tsample_doc6.txt 1\n",
      "これ\tsample_doc5.txt 1\n",
      "これ\tsample_doc8.txt 1\n",
      "これから\tsample_doc10.txt 1\n",
      "こん\tsample_doc10.txt 1\n",
      "こんな\tsample_doc4.txt 1\n",
      "さえ\tsample_doc7.txt 1\n",
      "さて\tsample_doc10.txt 1\n",
      "さらさら\tsample_doc9.txt 1\n",
      "さり\tsample_doc6.txt 1\n",
      "しかし\tsample_doc2.txt 1\n",
      "しかも\tsample_doc2.txt 1\n",
      "しき\tsample_doc10.txt 1\n",
      "しばらく\tsample_doc6.txt 2\n",
      "しばらく\tsample_doc8.txt 1\n",
      "しまっ\tsample_doc7.txt 1\n",
      "じめじめ\tsample_doc1.txt 1\n",
      "する\tsample_doc10.txt 1\n",
      "する\tsample_doc6.txt 1\n",
      "そう\tsample_doc10.txt 1\n",
      "そう\tsample_doc2.txt 1\n",
      "そうして\tsample_doc5.txt 1\n",
      "そこ\tsample_doc9.txt 1\n",
      "その\tsample_doc10.txt 1\n",
      "その\tsample_doc2.txt 1\n",
      "その\tsample_doc5.txt 1\n",
      "その\tsample_doc7.txt 1\n",
      "そのうち\tsample_doc9.txt 1\n",
      "その後\tsample_doc4.txt 1\n",
      "それ\tsample_doc2.txt 1\n",
      "それ\tsample_doc6.txt 1\n",
      "そろ\tsample_doc9.txt 1\n",
      "そろりと\tsample_doc9.txt 1\n",
      "たく\tsample_doc9.txt 1\n",
      "たくさん\tsample_doc7.txt 1\n",
      "ただ\tsample_doc3.txt 1\n",
      "たばこ\tsample_doc5.txt 1\n",
      "たら\tsample_doc8.txt 2\n",
      "たら\tsample_doc9.txt 1\n",
      "だいぶ\tsample_doc4.txt 1\n",
      "だけ\tsample_doc1.txt 1\n",
      "だけ\tsample_doc6.txt 1\n",
      "ついに\tsample_doc10.txt 1\n",
      "つか\tsample_doc1.txt 1\n",
      "つかまえ\tsample_doc2.txt 1\n",
      "つるつる\tsample_doc4.txt 1\n",
      "でも\tsample_doc1.txt 1\n",
      "でも\tsample_doc3.txt 1\n",
      "でも\tsample_doc7.txt 1\n",
      "とある\tsample_doc9.txt 1\n",
      "とうてい\tsample_doc6.txt 1\n",
      "として\tsample_doc6.txt 1\n",
      "とにかく\tsample_doc10.txt 1\n",
      "とんと\tsample_doc1.txt 1\n",
      "どう\tsample_doc8.txt 1\n",
      "どうして\tsample_doc10.txt 1\n",
      "どうにか\tsample_doc9.txt 1\n",
      "どうも\tsample_doc5.txt 1\n",
      "どうも\tsample_doc9.txt 1\n",
      "どこ\tsample_doc1.txt 1\n",
      "ない\tsample_doc10.txt 2\n",
      "ない\tsample_doc4.txt 1\n",
      "ない\tsample_doc6.txt 3\n",
      "ない\tsample_doc7.txt 1\n",
      "ない\tsample_doc8.txt 2\n",
      "ない\tsample_doc9.txt 2\n",
      "なかっ\tsample_doc10.txt 1\n",
      "なかっ\tsample_doc2.txt 2\n",
      "なく\tsample_doc10.txt 1\n",
      "なっ\tsample_doc10.txt 2\n",
      "なら\tsample_doc10.txt 1\n",
      "なら\tsample_doc4.txt 1\n",
      "なり\tsample_doc10.txt 1\n",
      "なる\tsample_doc10.txt 1\n",
      "なる\tsample_doc6.txt 1\n",
      "なる\tsample_doc9.txt 1\n",
      "のそのそ\tsample_doc7.txt 1\n",
      "のみ\tsample_doc4.txt 1\n",
      "はい出し\tsample_doc7.txt 1\n",
      "はじめ\tsample_doc3.txt 1\n",
      "はず\tsample_doc4.txt 1\n",
      "はっ\tsample_doc9.txt 1\n",
      "はてな\tsample_doc7.txt 1\n",
      "ばかり\tsample_doc3.txt 1\n",
      "ひだり\tsample_doc9.txt 1\n",
      "ひらに\tsample_doc3.txt 1\n",
      "ぴき\tsample_doc7.txt 1\n",
      "ふと\tsample_doc7.txt 1\n",
      "ふん\tsample_doc8.txt 1\n",
      "ぷうぷうと\tsample_doc5.txt 1\n",
      "べき\tsample_doc4.txt 1\n",
      "べつ\tsample_doc8.txt 1\n",
      "ぼう\tsample_doc10.txt 1\n",
      "ぽく\tsample_doc5.txt 1\n",
      "また\tsample_doc8.txt 1\n",
      "まだ\tsample_doc1.txt 1\n",
      "まで\tsample_doc10.txt 1\n",
      "まで\tsample_doc6.txt 1\n",
      "まで\tsample_doc7.txt 1\n",
      "まで\tsample_doc9.txt 1\n",
      "まるで\tsample_doc4.txt 1\n",
      "むせ\tsample_doc5.txt 1\n",
      "むやみ\tsample_doc6.txt 1\n",
      "むやみ\tsample_doc7.txt 1\n",
      "もう\tsample_doc10.txt 1\n",
      "もぐり込ん\tsample_doc9.txt 1\n",
      "もし\tsample_doc10.txt 1\n",
      "もの\tsample_doc1.txt 1\n",
      "もの\tsample_doc10.txt 2\n",
      "もの\tsample_doc3.txt 2\n",
      "もの\tsample_doc5.txt 1\n",
      "もの\tsample_doc9.txt 1\n",
      "ものの\tsample_doc10.txt 1\n",
      "やっ\tsample_doc8.txt 1\n",
      "やら\tsample_doc6.txt 1\n",
      "ゆう\tsample_doc10.txt 1\n",
      "よい\tsample_doc6.txt 1\n",
      "よい\tsample_doc9.txt 1\n",
      "よう\tsample_doc7.txt 1\n",
      "ようやく\tsample_doc5.txt 1\n",
      "ようやく\tsample_doc8.txt 1\n",
      "ようやく\tsample_doc9.txt 1\n",
      "よかろ\tsample_doc8.txt 1\n",
      "よく\tsample_doc10.txt 1\n",
      "られ\tsample_doc3.txt 2\n",
      "られ\tsample_doc7.txt 2\n",
      "わに\tsample_doc4.txt 1\n",
      "わら\tsample_doc7.txt 1\n",
      "をもって\tsample_doc4.txt 1\n",
      "スー\tsample_doc3.txt 1\n",
      "ニャー\tsample_doc8.txt 2\n",
      "ニャーニャー\tsample_doc1.txt 1\n",
      "フワフワ\tsample_doc3.txt 1\n",
      "一\tsample_doc4.txt 2\n",
      "一刻\tsample_doc10.txt 1\n",
      "一樹\tsample_doc10.txt 1\n",
      "一番\tsample_doc2.txt 1\n",
      "一疋\tsample_doc7.txt 1\n",
      "三\tsample_doc10.txt 1\n",
      "上\tsample_doc3.txt 1\n",
      "上\tsample_doc7.txt 1\n",
      "上\tsample_doc9.txt 1\n",
      "上今\tsample_doc7.txt 1\n",
      "不思議\tsample_doc10.txt 1\n",
      "中\tsample_doc2.txt 1\n",
      "中\tsample_doc5.txt 1\n",
      "中\tsample_doc7.txt 1\n",
      "事\tsample_doc4.txt 1\n",
      "事\tsample_doc5.txt 1\n",
      "事\tsample_doc6.txt 1\n",
      "事\tsample_doc9.txt 1\n",
      "云\tsample_doc10.txt 1\n",
      "人間\tsample_doc1.txt 1\n",
      "人間\tsample_doc2.txt 1\n",
      "人間\tsample_doc3.txt 1\n",
      "人間\tsample_doc5.txt 1\n",
      "人間\tsample_doc9.txt 1\n",
      "今\tsample_doc3.txt 1\n",
      "今日\tsample_doc10.txt 1\n",
      "仕方\tsample_doc10.txt 1\n",
      "仕方\tsample_doc9.txt 1\n",
      "付い\tsample_doc7.txt 1\n",
      "何\tsample_doc1.txt 1\n",
      "何\tsample_doc2.txt 1\n",
      "何\tsample_doc6.txt 1\n",
      "何\tsample_doc7.txt 1\n",
      "何\tsample_doc9.txt 1\n",
      "何だか\tsample_doc3.txt 1\n",
      "何となく\tsample_doc9.txt 1\n",
      "兄弟\tsample_doc7.txt 1\n",
      "先\tsample_doc10.txt 1\n",
      "内\tsample_doc9.txt 1\n",
      "出\tsample_doc6.txt 1\n",
      "出\tsample_doc8.txt 1\n",
      "出\tsample_doc9.txt 2\n",
      "出会\tsample_doc4.txt 1\n",
      "出来\tsample_doc10.txt 1\n",
      "分ら\tsample_doc10.txt 1\n",
      "分ら\tsample_doc6.txt 2\n",
      "分別\tsample_doc8.txt 1\n",
      "別に\tsample_doc8.txt 1\n",
      "別段\tsample_doc2.txt 1\n",
      "到底\tsample_doc6.txt 1\n",
      "前\tsample_doc8.txt 1\n",
      "助から\tsample_doc6.txt 1\n",
      "動く\tsample_doc6.txt 2\n",
      "名前\tsample_doc1.txt 1\n",
      "向う\tsample_doc8.txt 1\n",
      "吹く\tsample_doc5.txt 1\n",
      "吾輩\tsample_doc1.txt 2\n",
      "吾輩\tsample_doc10.txt 2\n",
      "吾輩\tsample_doc7.txt 1\n",
      "吾輩\tsample_doc8.txt 1\n",
      "咽\tsample_doc5.txt 1\n",
      "善い\tsample_doc10.txt 1\n",
      "坐っ\tsample_doc6.txt 1\n",
      "坐っ\tsample_doc8.txt 1\n",
      "垣根\tsample_doc10.txt 1\n",
      "声\tsample_doc9.txt 1\n",
      "大きな\tsample_doc8.txt 1\n",
      "妙\tsample_doc3.txt 1\n",
      "始\tsample_doc3.txt 1\n",
      "始め\tsample_doc1.txt 1\n",
      "始め\tsample_doc6.txt 1\n",
      "始め\tsample_doc9.txt 1\n",
      "始末\tsample_doc10.txt 1\n",
      "姿\tsample_doc7.txt 1\n",
      "実に\tsample_doc5.txt 1\n",
      "容子\tsample_doc7.txt 1\n",
      "寒\tsample_doc10.txt 1\n",
      "寒し\tsample_doc10.txt 1\n",
      "少し\tsample_doc3.txt 1\n",
      "崩\tsample_doc9.txt 1\n",
      "左\tsample_doc9.txt 1\n",
      "度\tsample_doc4.txt 1\n",
      "廻り\tsample_doc9.txt 1\n",
      "廻る\tsample_doc6.txt 1\n",
      "弱っ\tsample_doc5.txt 1\n",
      "当時\tsample_doc2.txt 1\n",
      "彼\tsample_doc3.txt 1\n",
      "心持\tsample_doc6.txt 1\n",
      "忍び込ん\tsample_doc10.txt 1\n",
      "思い\tsample_doc8.txt 1\n",
      "思っ\tsample_doc3.txt 1\n",
      "思っ\tsample_doc6.txt 1\n",
      "思っ\tsample_doc9.txt 1\n",
      "思わ\tsample_doc2.txt 1\n",
      "急\tsample_doc7.txt 1\n",
      "恐し\tsample_doc2.txt 1\n",
      "悪く\tsample_doc6.txt 1\n",
      "感じ\tsample_doc3.txt 2\n",
      "我々\tsample_doc2.txt 1\n",
      "我慢\tsample_doc9.txt 1\n",
      "所\tsample_doc1.txt 1\n",
      "所\tsample_doc7.txt 1\n",
      "所\tsample_doc9.txt 2\n",
      "持ち上げ\tsample_doc3.txt 1\n",
      "捕\tsample_doc2.txt 1\n",
      "掌\tsample_doc3.txt 2\n",
      "掌\tsample_doc6.txt 1\n",
      "方\tsample_doc10.txt 2\n",
      "日\tsample_doc9.txt 1\n",
      "明い\tsample_doc7.txt 1\n",
      "明るい\tsample_doc7.txt 1\n",
      "明るく\tsample_doc10.txt 1\n",
      "時\tsample_doc10.txt 1\n",
      "時\tsample_doc3.txt 2\n",
      "時々\tsample_doc2.txt 1\n",
      "時々\tsample_doc5.txt 1\n",
      "暖か\tsample_doc10.txt 1\n",
      "暗\tsample_doc6.txt 1\n",
      "暗\tsample_doc7.txt 1\n",
      "暗く\tsample_doc10.txt 1\n",
      "暮れ\tsample_doc9.txt 1\n",
      "書生\tsample_doc2.txt 2\n",
      "書生\tsample_doc3.txt 1\n",
      "書生\tsample_doc6.txt 2\n",
      "書生\tsample_doc7.txt 1\n",
      "書生\tsample_doc8.txt 1\n",
      "来\tsample_doc8.txt 2\n",
      "来\tsample_doc9.txt 1\n",
      "来る\tsample_doc10.txt 1\n",
      "棄て\tsample_doc7.txt 1\n",
      "残っ\tsample_doc3.txt 1\n",
      "母親\tsample_doc7.txt 1\n",
      "毛\tsample_doc10.txt 1\n",
      "毛\tsample_doc4.txt 1\n",
      "気\tsample_doc7.txt 1\n",
      "池\tsample_doc8.txt 2\n",
      "池\tsample_doc9.txt 2\n",
      "決心\tsample_doc9.txt 1\n",
      "泣い\tsample_doc1.txt 1\n",
      "泣い\tsample_doc8.txt 1\n",
      "泣き\tsample_doc9.txt 1\n",
      "減っ\tsample_doc9.txt 1\n",
      "減る\tsample_doc10.txt 1\n",
      "渡っ\tsample_doc9.txt 1\n",
      "火\tsample_doc6.txt 1\n",
      "無\tsample_doc6.txt 1\n",
      "無\tsample_doc7.txt 1\n",
      "無い\tsample_doc1.txt 1\n",
      "無理やり\tsample_doc9.txt 1\n",
      "煙\tsample_doc5.txt 1\n",
      "煙草\tsample_doc5.txt 1\n",
      "煮\tsample_doc2.txt 1\n",
      "片\tsample_doc4.txt 1\n",
      "猫\tsample_doc1.txt 1\n",
      "猫\tsample_doc4.txt 1\n",
      "猶予\tsample_doc10.txt 1\n",
      "獰悪\tsample_doc2.txt 1\n",
      "生れ\tsample_doc1.txt 1\n",
      "痛い\tsample_doc7.txt 1\n",
      "真中\tsample_doc4.txt 1\n",
      "眼\tsample_doc6.txt 2\n",
      "眼\tsample_doc7.txt 1\n",
      "知っ\tsample_doc5.txt 1\n",
      "知れ\tsample_doc10.txt 1\n",
      "破れ\tsample_doc10.txt 1\n",
      "種族\tsample_doc2.txt 1\n",
      "穴\tsample_doc10.txt 1\n",
      "穴\tsample_doc5.txt 1\n",
      "穴\tsample_doc9.txt 1\n",
      "突起\tsample_doc4.txt 1\n",
      "竹垣\tsample_doc10.txt 1\n",
      "竹垣\tsample_doc9.txt 1\n",
      "第\tsample_doc4.txt 1\n",
      "笹原\tsample_doc7.txt 1\n",
      "笹原\tsample_doc8.txt 1\n",
      "縁\tsample_doc10.txt 1\n",
      "考\tsample_doc2.txt 1\n",
      "考え\tsample_doc8.txt 1\n",
      "考え付い\tsample_doc8.txt 1\n",
      "考え出そ\tsample_doc6.txt 1\n",
      "聞く\tsample_doc2.txt 1\n",
      "肝心\tsample_doc7.txt 1\n",
      "胸\tsample_doc6.txt 1\n",
      "腹\tsample_doc10.txt 1\n",
      "腹\tsample_doc9.txt 1\n",
      "自分\tsample_doc6.txt 1\n",
      "臭い\tsample_doc9.txt 1\n",
      "至る\tsample_doc10.txt 1\n",
      "苦しい\tsample_doc9.txt 1\n",
      "落ちつい\tsample_doc3.txt 1\n",
      "蔭\tsample_doc10.txt 1\n",
      "薄暗い\tsample_doc1.txt 1\n",
      "薬缶\tsample_doc4.txt 1\n",
      "藁\tsample_doc7.txt 1\n",
      "行く\tsample_doc10.txt 1\n",
      "行く\tsample_doc9.txt 1\n",
      "装飾\tsample_doc4.txt 1\n",
      "裏\tsample_doc6.txt 1\n",
      "見\tsample_doc1.txt 1\n",
      "見\tsample_doc3.txt 2\n",
      "見\tsample_doc8.txt 2\n",
      "見え\tsample_doc7.txt 1\n",
      "見る\tsample_doc7.txt 2\n",
      "見当\tsample_doc1.txt 1\n",
      "記憶\tsample_doc1.txt 1\n",
      "記憶\tsample_doc6.txt 1\n",
      "訪問\tsample_doc10.txt 1\n",
      "試み\tsample_doc8.txt 1\n",
      "話\tsample_doc2.txt 1\n",
      "誰\tsample_doc8.txt 1\n",
      "路傍\tsample_doc10.txt 1\n",
      "載せ\tsample_doc3.txt 1\n",
      "輪\tsample_doc4.txt 1\n",
      "迎\tsample_doc8.txt 1\n",
      "這\tsample_doc7.txt 1\n",
      "這\tsample_doc9.txt 1\n",
      "這い出す\tsample_doc8.txt 1\n",
      "這入\tsample_doc9.txt 1\n",
      "通路\tsample_doc10.txt 1\n",
      "速力\tsample_doc6.txt 1\n",
      "逢\tsample_doc4.txt 1\n",
      "運転\tsample_doc6.txt 1\n",
      "違っ\tsample_doc7.txt 1\n",
      "邸\tsample_doc10.txt 1\n",
      "邸\tsample_doc9.txt 1\n",
      "降っ\tsample_doc10.txt 1\n",
      "隠し\tsample_doc7.txt 1\n",
      "隣家\tsample_doc10.txt 1\n",
      "雨\tsample_doc10.txt 1\n",
      "非常\tsample_doc6.txt 1\n",
      "非常\tsample_doc7.txt 1\n",
      "非常\tsample_doc9.txt 2\n",
      "音\tsample_doc6.txt 1\n",
      "頃\tsample_doc5.txt 1\n",
      "顔\tsample_doc3.txt 1\n",
      "顔\tsample_doc4.txt 2\n",
      "風\tsample_doc9.txt 1\n",
      "食う\tsample_doc2.txt 1\n",
      "食物\tsample_doc9.txt 1\n",
      "飲む\tsample_doc5.txt 1\n",
      "餓死\tsample_doc10.txt 1\n"
     ]
    }
   ],
   "source": [
    "# 必要なライブラリを読み込む\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "t = Tokenizer()\n",
    "\n",
    "# データフォルダの設定\n",
    "DATA = \"../data\"\n",
    "\n",
    "# 辞書オブジェクトの初期化\n",
    "dict = {}\n",
    "\n",
    "# 不要語としてマッチしたいパターンの定義\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "\n",
    "# 不要語辞書の初期化\n",
    "stopwords = {}\n",
    "\n",
    "# 不要語の追加\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "      \n",
    "# DATAフォルダに含まれるファイルを一つずつ処理\n",
    "for filename in os.listdir(DATA):\n",
    "    f = open(DATA + '/' + filename, 'r')\n",
    "\n",
    "    # ファイルを1行ずつ処理\n",
    "    for line in f:\n",
    "        tokens = t.tokenize(line)\n",
    "\n",
    "        # 分かち書きした単語を一つずつ処理\n",
    "        for token in tokens:\n",
    "        \n",
    "\n",
    "            # 不要語のスキップ（正規表現）\n",
    "            if pattern.match(token.surface):\n",
    "                continue\n",
    "\n",
    "            # 不要語のスキップ（不要語辞書）\n",
    "            if token.surface in stopwords:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # 索引語の追加\n",
    "            if token.surface in dict:\n",
    "                # もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "                if filename in dict[token.surface]:\n",
    "                    dict[token.surface][filename] += 1\n",
    "\n",
    "                # さもなくば\n",
    "                else:\n",
    "                    dict[token.surface][filename] = 1\n",
    "\n",
    "            # さもなくば\n",
    "            else:\n",
    "                dict[token.surface] = {}\n",
    "                dict[token.surface][filename] = 1\n",
    "\n",
    "    # ファイルを閉じる\n",
    "    f.close()\n",
    "\n",
    "# 全体の出力（ソート）\n",
    "for key in sorted(dict):\n",
    "    #print(key + '\\t' + str(dict[key]))\n",
    "    for elm in sorted(dict[key]):\n",
    "        print(key + '\\t' + elm + \" \" + str(dict[key][elm]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> `あっ\tsample_doc2.txt\t1`  \n",
    "> `あっ\tsample_doc3.txt\t1`  \n",
    "> `あっ\tsample_doc4.txt\t1`  \n",
    "> `あと\tsample_doc2.txt\t1`  \n",
    "> `あと\tsample_doc6.txt\t1`  \n",
    "> ...\n",
    "\n",
    "\n",
    "という感じで、句読点や一文字のひらがなが除かれた一覧が出力されたら成功です。\n",
    "\n",
    "> 不要語辞書に不要語を追加してみて、出力が変わるかを確認しましょう。  \n",
    "> しかしその後で、不要語の定義を**元に戻すのを忘れないで**ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 索引ファイルの保存\n",
    "\n",
    "それでは、完成した索引語一覧をファイルに保存しましょう。\n",
    "\n",
    "> `index`フォルダが正しい位置に作成済みであることを再確認しましょう！\n",
    "\n",
    "これまで`print`メソッドを使って出力していた内容をファイルに書き込むには、`write`メソッドを使います。\n",
    "\n",
    "> `write`メソッドは文末に改行文字（`\\\\n`）を自動的に付与しないので、改行が必要な場合は明示的に追加する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 索引語出力用ファイル名の設定\n",
    "INDEX = \"../index\"\n",
    "outfilename = INDEX + \"/index.txt\"\n",
    "\n",
    "# 出力用ファイルを作成\n",
    "f = open(outfilename, 'w')\n",
    "# 行を書き込む\n",
    "f.write('内容' + '\\n')\n",
    "# ファイルを閉じる\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "上のプログラムのポイントは、`outfilename`が参照するファイルを**書き込みモード**（`w`）で開いているところです。これまでのプログラムでは、読み込み専用モード（`r`）で開いていました。\n",
    "\n",
    "> 書き込みモードは、開く度に存在していた**中身を消去して**、新しく書き込むことに注意してください。  \n",
    "> 既存の内容に足して書き込みたい場合は、**追記モード**（`a`）で開きます。\n",
    "\n",
    "また、書き込みモードや追記モードで開いたファイルは、処理が完了したら`close()`メソッドを使って、閉じておくのが**行儀良い作法**とされていますので、実践しましょう。\n",
    "\n",
    "それでは、これまで`print`メソッドで出力していた部分を、`write`メソッドを使ってファイルに書き込む処理に差し替えたプログラムを書いてみましょう\n",
    "\n",
    "> 出力用ファイル名の設定や出力用ファイルの作成は、出力用の**`for`ループに入る前**に行う必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリを読み込む\n",
    "import os\n",
    "from janome.tokenizer import Tokenizer\n",
    "import re\n",
    "t = Tokenizer()\n",
    "\n",
    "# データフォルダの設定\n",
    "DATA = \"../data\"\n",
    "\n",
    "# 辞書オブジェクトの初期化\n",
    "dict = {}\n",
    "\n",
    "# 不要語としてマッチしたいパターンの定義\n",
    "pattern = re.compile(r\"^[　-ー]$\")\n",
    "\n",
    "# 不要語辞書の初期化\n",
    "stopwords = {}\n",
    "\n",
    "# 不要語の追加\n",
    "stopwords['という'] = 1\n",
    "stopwords['にて'] = 1\n",
    "\n",
    "      \n",
    "# DATAフォルダに含まれるファイルを一つずつ処理\n",
    "for filename in os.listdir(DATA):\n",
    "    f = open(DATA + '/' + filename, 'r')\n",
    "\n",
    "    # ファイルを1行ずつ処理\n",
    "    for line in f:\n",
    "        tokens = t.tokenize(line)\n",
    "\n",
    "        # 分かち書きした単語を一つずつ処理\n",
    "        for token in tokens:\n",
    "        \n",
    "\n",
    "            # 不要語のスキップ（正規表現）\n",
    "            if pattern.match(token.surface):\n",
    "                continue\n",
    "\n",
    "            # 不要語のスキップ（不要語辞書）\n",
    "            if token.surface in stopwords:\n",
    "                continue\n",
    "            \n",
    "\n",
    "            # 索引語の追加\n",
    "            if token.surface in dict:\n",
    "                # もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "                if filename in dict[token.surface]:\n",
    "                    dict[token.surface][filename] += 1\n",
    "\n",
    "                # さもなくば\n",
    "                else:\n",
    "                    dict[token.surface][filename] = 1\n",
    "\n",
    "            # さもなくば\n",
    "            else:\n",
    "                dict[token.surface] = {}\n",
    "                dict[token.surface][filename] = 1\n",
    "\n",
    "    # ファイルを閉じる\n",
    "    f.close()\n",
    "\n",
    "# 全体の出力（ソート）\n",
    "\n",
    "# 索引語出力用ファイル名の設定\n",
    "INDEX = \"../index\"\n",
    "outfilename = INDEX + \"/index.txt\"\n",
    "\n",
    "# 出力用ファイルを作成\n",
    "f = open(outfilename, 'w')\n",
    "# 行を書き込む\n",
    "for key in sorted(dict):\n",
    "    #print(key + '\\t' + str(dict[key]))\n",
    "    for elm in sorted(dict[key]):\n",
    "        f.write(key + '\\t' + elm + '\\t' + str(dict[key][elm]))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "# ファイルを閉じる\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**エラーが出力されずに**、上の入手力ブロックが緑色から青色に変化したら処理が終了した印です。画面には特に何も表示されません。\n",
    "\n",
    "`~/CJE3/index/`フォルダに、`index.txt`というファイルが作成され、その中に、索引語一覧が書き込まれているか確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### プログラムの説明文を書く\n",
    "\n",
    "それでは、本ノートブックの最後の演習として、索引ファイルを生成するプログラムの説明文を書きましょう。説明文を書く上での注意点は前回のノートブックを参照してください。\n",
    "\n",
    "\n",
    "> 2: ディレクトリ内にあるファイルを読み込むためのosライブラリをimport\n",
    "\n",
    "> 3: 分かち書きをするライブラリjanome.tokenizerから、分かち書きクラスTokenizerを読み込む\n",
    "\n",
    "> 4: 正規表現をあつかうreパッケージの読み込み\n",
    "\n",
    "> 5: 分かち書きクラスTokenizerを初期化し、クラスのショートカットを変数tに格納する\n",
    "\n",
    "> 7: CJE3のdataフォルダを指定\n",
    "\n",
    "> 11: dictオブジェクトの初期化\n",
    "\n",
    "> 14: 不用語として削除したいパターン語の定義\n",
    "\n",
    "> 17: 不要語辞書の初期化\n",
    "\n",
    "> 20~21: 不要語の追加\n",
    "\n",
    "> 25: osライブラリのlistdirメソッドを使って、変数DATAが参照するフォルダに含まれるファイル名のリストを取得。最初のファイル名を変数filenameに格納。forメソッドを使って以下の処理をリストの終わりまで繰り返す\n",
    "\n",
    "> 26: 変数filenameをopenメソッドを使って読み出し専用モード（'r'）で開き、開いたファイルを後から処理できるように、ファイルのショートカットを変数fに格納する\n",
    "\n",
    "> 29: 変数fの内容を先頭から１行取り出し、変数lineに格納する。これを最終行まで繰り返す。\n",
    "\n",
    "> 30: 変数lineに格納された行を、tokenizeメソッドで分かち書きして、その結果を変数tokensに格納\n",
    "\n",
    "> 31: 変数tokensに格納された要素を変数tokenに格納する。これを最後の要素になるまで繰り返す。\n",
    "\n",
    "> 36: token.surfaceが正規表現にマッチした場合の処理\n",
    "\n",
    "> 41: token.surfaceが不要語辞書に含まれていた場合の処理\n",
    "\n",
    "> 46: もしdictに単語をキーとするレコードが存在していれば\n",
    "\n",
    "> 48: もし、そのキーの値が参照する無名辞書オブジェクトにファイル名をキーとするレコードが存在していれば\n",
    "\n",
    "> 49: 出現頻度を１増やす\n",
    "\n",
    "> 52: 48行目がfalseだった場合\n",
    "\n",
    "> 53: 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "> 56: 46行目がfalseだった場合\n",
    "\n",
    "> 57: そのキーを使ってdictに無名辞書オブジェクトを初期化\n",
    "\n",
    "> 58: 単語語とファイル名をキーとして頻度1を値に挿入\n",
    "\n",
    "> 61: ファイルを閉じる\n",
    "\n",
    "> 66: CJE3のindexフォルダを指定\n",
    "\n",
    "> 67: 出力ファイルを変数outfilenameに保存\n",
    "\n",
    "> 72: for文を使って、辞書オブジェクトをソートし、辞書オブジェクトのキーを順番に変数keyに格納。以下の処理をキーがなくなるまで繰り返す。\n",
    "\n",
    "> 74: for文を使って、無名辞書の値を変数elmに格納。以下の処理をキーがなくなるまで繰り返す。\n",
    "\n",
    "> 75~76: 出力用ファイルに書き出し\n",
    "\n",
    "> 79: ファイルを閉じる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ここまで完成したら、TAか教員に確認してもらってから、ノートブック3にいきましょう。\n",
    "\n",
    "### チェックポイント\n",
    "\n",
    "- プログラムが期待通りに動作するか、索引ファイルは作成されているか\n",
    "- ノートブックで説明したやり方に沿ってプログラムが書かれているか（我流で書いていないか）\n",
    "- `re`が何をするために必要なライブラリか説明できるか\n",
    "- 不要語除去の正規表現`r\" ... \"`の照合ルールを正確に説明できるか\n",
    "- 入れ子の辞書オブジェクト生成手順について正確に説明できるか\n",
    "- プログラム末尾に`f.close()`を忘れていないか\n",
    "- 説明文にライブラリ・クラス・変数・メソッド・ループなどのキーワードを正確に使用しているか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Copyright &copy; 2019. Hideo Joho and Haitao Yu. All rights reserved.  \n",
    "無断複製・転載・配布行為を禁止します。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "354px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
